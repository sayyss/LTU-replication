{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d904a05b-9a7d-4e55-b5e8-d94f69d0020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "import torchaudio\n",
    "from Cave_model import CAVMAEFTAudio\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "import torch.nn.init as init\n",
    "from transformers.modeling_outputs import BaseModelOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8edd18d-dc26-4abe-bb6e-9dcaef4e75bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sayyss/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b696eab-249f-4adc-90e0-b1f92ae530fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb49928-847b-40a5-904e-3ef0234e2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3343f990-9a17-4126-8fd7-5a0d0eb041a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomT5ForConditionalGeneration(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.audio_encoder = CAVMAEFTAudio()\n",
    "        self.audio_proj = nn.Sequential(nn.LayerNorm(768, elementwise_affine=False), nn.Linear(768, 1024))\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        audio_input,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        decoder_attention_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # cut decoder_input_ids if past_key_values is used\n",
    "        if past_key_values is not None:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "\n",
    "            # Some generation methods already pass only the last input ID\n",
    "            if input_ids.shape[1] > past_length:\n",
    "                remove_prefix_length = past_length\n",
    "            else:\n",
    "                # Default to old behavior: keep only final ID\n",
    "                remove_prefix_length = input_ids.shape[1] - 1\n",
    "\n",
    "            input_ids = input_ids[:, remove_prefix_length:]\n",
    "        \n",
    "        return {\n",
    "            \"decoder_input_ids\": input_ids,\n",
    "            \"audio_input\": audio_input,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"decoder_attention_mask\": decoder_attention_mask,\n",
    "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        audio_input = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n",
    "            config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n",
    "            labels in `[0, ..., config.vocab_size]`\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "        >>> model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "        >>> # training\n",
    "        >>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "        >>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "        >>> outputs = model(input_ids=input_ids, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\n",
    "        >>> # inference\n",
    "        >>> input_ids = tokenizer(\n",
    "        ...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    "        ... ).input_ids  # Batch size 1\n",
    "        >>> outputs = model.generate(input_ids)\n",
    "        >>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        >>> # studies have shown that owning a dog is good for you.\n",
    "        ```\"\"\"\n",
    "        \n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
    "        if head_mask is not None and decoder_head_mask is None:\n",
    "            if self.config.num_layers == self.config.num_decoder_layers:\n",
    "                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n",
    "                decoder_head_mask = head_mask\n",
    "\n",
    "        # ******** Custom modifications start *********\n",
    "        # Get audio embeddings\n",
    "        if audio_input == None:\n",
    "            raise ValueError(\"audio input cannot be empty\")\n",
    "            \n",
    "        audio_input = self.audio_encoder(audio_input)  # [B, 512, 768]\n",
    "        audio_input = audio_input.reshape(audio_input.shape[0], 8, 64, audio_input.shape[-1])\n",
    "        audio_input = torch.mean(audio_input, dim=1)  # mean pool over the frequency dimension # [B, 64, 768]\n",
    "        audio_input = torch.nn.functional.avg_pool2d(audio_input, (2, 1)) #[B, 32, 768]\n",
    "        # hard norm to 50\n",
    "        audio_input = audio_input / 50\n",
    "        audio_input = self.audio_proj(audio_input) #[B, 32, 1024]\n",
    "        audio_length = audio_input.shape[1]\n",
    "        audio_embeds = audio_input.to(device) # [2,32,1024] \n",
    "        \n",
    "        # Custom: get embeddings\n",
    "        # Only runs during training\n",
    "        if inputs_embeds is None and decoder_input_ids is None and input_ids is not None:\n",
    "            \n",
    "            inputs_embeds = self.shared(input_ids).to(device) # [b, seq_length, 768]\n",
    "            seq_length = audio_length + inputs_embeds.shape[1] # [32+seq_length]\n",
    "            \n",
    "            inputs_embeds = torch.cat((inputs_embeds, audio_embeds), dim=1)  # Shape: (2,sequence_length + 32, 1024)\n",
    "\n",
    "            \"\"\"\n",
    "            max_length = 512\n",
    "            seq_length = inputs_embeds.size(1)  \n",
    "            padding_length = max_length - seq_length\n",
    "            \n",
    "            # Truncate if the sequence length exceeds max_length\n",
    "            if seq_length > max_length:\n",
    "                inputs_embeds = inputs_embeds[:, :max_length, :]\n",
    "            \n",
    "            # Apply padding if the sequence is shorter than max_length\n",
    "            if padding_length > 0:\n",
    "                padding_tensor = torch.zeros((inputs_embeds.size(0), padding_length, inputs_embeds.size(2))).to(device)\n",
    "                inputs_embeds = torch.cat((inputs_embeds, padding_tensor), dim=1) \n",
    "            \n",
    "    \n",
    "            attention_mask = torch.ones((inputs_embeds.size(0), inputs_embeds.size(1))).to(device)  # Match sequence dimension\n",
    "            \"\"\"\n",
    "            \n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        if encoder_outputs is None:\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=None, # Custom: change to none because we already defined embeddings\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        hidden_states = encoder_outputs[0]\n",
    "\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "\n",
    "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            # get decoder inputs from shifting lm labels to the right\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "            hidden_states = hidden_states.to(self.decoder.first_device)\n",
    "            if decoder_input_ids is not None:\n",
    "                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.decoder.first_device)\n",
    "            if decoder_attention_mask is not None:\n",
    "                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
    "\n",
    "        # modifications for inference\n",
    "        # only runs when inference, skips to using labels as decoder input when training instead\n",
    "        if decoder_input_ids is not None and labels is None and input_ids is None:\n",
    "            decoder_inputs_embeds = self.shared(decoder_input_ids).to(device)\n",
    "            decoder_inputs_embeds = torch.cat((decoder_inputs_embeds, audio_embeds), dim=1)\n",
    "            decoder_input_ids = None\n",
    "            \n",
    "            \n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = decoder_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.encoder.first_device)\n",
    "            self.lm_head = self.lm_head.to(self.encoder.first_device)\n",
    "            sequence_output = sequence_output.to(self.lm_head.weight.device)\n",
    "\n",
    "        if self.config.tie_word_embeddings:\n",
    "            # Rescale output before projecting on vocab\n",
    "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "            sequence_output = sequence_output * (self.model_dim**-0.5)\n",
    "\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            # move labels to correct device to enable PP\n",
    "            labels = labels.to(lm_logits.device)\n",
    "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e8489-edf1-4f6b-83e2-314e004d6f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f59470-b3b8-4e11-87e5-2c5291e33ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/sayyss/.conda/envs/LTU-Replication/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of CustomT5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['audio_encoder.blocks_a.0.attn.proj.bias', 'audio_encoder.blocks_a.0.attn.proj.weight', 'audio_encoder.blocks_a.0.attn.qkv.bias', 'audio_encoder.blocks_a.0.attn.qkv.weight', 'audio_encoder.blocks_a.0.mlp.fc1.bias', 'audio_encoder.blocks_a.0.mlp.fc1.weight', 'audio_encoder.blocks_a.0.mlp.fc2.bias', 'audio_encoder.blocks_a.0.mlp.fc2.weight', 'audio_encoder.blocks_a.0.norm1.bias', 'audio_encoder.blocks_a.0.norm1.weight', 'audio_encoder.blocks_a.0.norm1_a.bias', 'audio_encoder.blocks_a.0.norm1_a.weight', 'audio_encoder.blocks_a.0.norm1_v.bias', 'audio_encoder.blocks_a.0.norm1_v.weight', 'audio_encoder.blocks_a.0.norm2.bias', 'audio_encoder.blocks_a.0.norm2.weight', 'audio_encoder.blocks_a.0.norm2_a.bias', 'audio_encoder.blocks_a.0.norm2_a.weight', 'audio_encoder.blocks_a.0.norm2_v.bias', 'audio_encoder.blocks_a.0.norm2_v.weight', 'audio_encoder.blocks_a.1.attn.proj.bias', 'audio_encoder.blocks_a.1.attn.proj.weight', 'audio_encoder.blocks_a.1.attn.qkv.bias', 'audio_encoder.blocks_a.1.attn.qkv.weight', 'audio_encoder.blocks_a.1.mlp.fc1.bias', 'audio_encoder.blocks_a.1.mlp.fc1.weight', 'audio_encoder.blocks_a.1.mlp.fc2.bias', 'audio_encoder.blocks_a.1.mlp.fc2.weight', 'audio_encoder.blocks_a.1.norm1.bias', 'audio_encoder.blocks_a.1.norm1.weight', 'audio_encoder.blocks_a.1.norm1_a.bias', 'audio_encoder.blocks_a.1.norm1_a.weight', 'audio_encoder.blocks_a.1.norm1_v.bias', 'audio_encoder.blocks_a.1.norm1_v.weight', 'audio_encoder.blocks_a.1.norm2.bias', 'audio_encoder.blocks_a.1.norm2.weight', 'audio_encoder.blocks_a.1.norm2_a.bias', 'audio_encoder.blocks_a.1.norm2_a.weight', 'audio_encoder.blocks_a.1.norm2_v.bias', 'audio_encoder.blocks_a.1.norm2_v.weight', 'audio_encoder.blocks_a.10.attn.proj.bias', 'audio_encoder.blocks_a.10.attn.proj.weight', 'audio_encoder.blocks_a.10.attn.qkv.bias', 'audio_encoder.blocks_a.10.attn.qkv.weight', 'audio_encoder.blocks_a.10.mlp.fc1.bias', 'audio_encoder.blocks_a.10.mlp.fc1.weight', 'audio_encoder.blocks_a.10.mlp.fc2.bias', 'audio_encoder.blocks_a.10.mlp.fc2.weight', 'audio_encoder.blocks_a.10.norm1.bias', 'audio_encoder.blocks_a.10.norm1.weight', 'audio_encoder.blocks_a.10.norm1_a.bias', 'audio_encoder.blocks_a.10.norm1_a.weight', 'audio_encoder.blocks_a.10.norm1_v.bias', 'audio_encoder.blocks_a.10.norm1_v.weight', 'audio_encoder.blocks_a.10.norm2.bias', 'audio_encoder.blocks_a.10.norm2.weight', 'audio_encoder.blocks_a.10.norm2_a.bias', 'audio_encoder.blocks_a.10.norm2_a.weight', 'audio_encoder.blocks_a.10.norm2_v.bias', 'audio_encoder.blocks_a.10.norm2_v.weight', 'audio_encoder.blocks_a.2.attn.proj.bias', 'audio_encoder.blocks_a.2.attn.proj.weight', 'audio_encoder.blocks_a.2.attn.qkv.bias', 'audio_encoder.blocks_a.2.attn.qkv.weight', 'audio_encoder.blocks_a.2.mlp.fc1.bias', 'audio_encoder.blocks_a.2.mlp.fc1.weight', 'audio_encoder.blocks_a.2.mlp.fc2.bias', 'audio_encoder.blocks_a.2.mlp.fc2.weight', 'audio_encoder.blocks_a.2.norm1.bias', 'audio_encoder.blocks_a.2.norm1.weight', 'audio_encoder.blocks_a.2.norm1_a.bias', 'audio_encoder.blocks_a.2.norm1_a.weight', 'audio_encoder.blocks_a.2.norm1_v.bias', 'audio_encoder.blocks_a.2.norm1_v.weight', 'audio_encoder.blocks_a.2.norm2.bias', 'audio_encoder.blocks_a.2.norm2.weight', 'audio_encoder.blocks_a.2.norm2_a.bias', 'audio_encoder.blocks_a.2.norm2_a.weight', 'audio_encoder.blocks_a.2.norm2_v.bias', 'audio_encoder.blocks_a.2.norm2_v.weight', 'audio_encoder.blocks_a.3.attn.proj.bias', 'audio_encoder.blocks_a.3.attn.proj.weight', 'audio_encoder.blocks_a.3.attn.qkv.bias', 'audio_encoder.blocks_a.3.attn.qkv.weight', 'audio_encoder.blocks_a.3.mlp.fc1.bias', 'audio_encoder.blocks_a.3.mlp.fc1.weight', 'audio_encoder.blocks_a.3.mlp.fc2.bias', 'audio_encoder.blocks_a.3.mlp.fc2.weight', 'audio_encoder.blocks_a.3.norm1.bias', 'audio_encoder.blocks_a.3.norm1.weight', 'audio_encoder.blocks_a.3.norm1_a.bias', 'audio_encoder.blocks_a.3.norm1_a.weight', 'audio_encoder.blocks_a.3.norm1_v.bias', 'audio_encoder.blocks_a.3.norm1_v.weight', 'audio_encoder.blocks_a.3.norm2.bias', 'audio_encoder.blocks_a.3.norm2.weight', 'audio_encoder.blocks_a.3.norm2_a.bias', 'audio_encoder.blocks_a.3.norm2_a.weight', 'audio_encoder.blocks_a.3.norm2_v.bias', 'audio_encoder.blocks_a.3.norm2_v.weight', 'audio_encoder.blocks_a.4.attn.proj.bias', 'audio_encoder.blocks_a.4.attn.proj.weight', 'audio_encoder.blocks_a.4.attn.qkv.bias', 'audio_encoder.blocks_a.4.attn.qkv.weight', 'audio_encoder.blocks_a.4.mlp.fc1.bias', 'audio_encoder.blocks_a.4.mlp.fc1.weight', 'audio_encoder.blocks_a.4.mlp.fc2.bias', 'audio_encoder.blocks_a.4.mlp.fc2.weight', 'audio_encoder.blocks_a.4.norm1.bias', 'audio_encoder.blocks_a.4.norm1.weight', 'audio_encoder.blocks_a.4.norm1_a.bias', 'audio_encoder.blocks_a.4.norm1_a.weight', 'audio_encoder.blocks_a.4.norm1_v.bias', 'audio_encoder.blocks_a.4.norm1_v.weight', 'audio_encoder.blocks_a.4.norm2.bias', 'audio_encoder.blocks_a.4.norm2.weight', 'audio_encoder.blocks_a.4.norm2_a.bias', 'audio_encoder.blocks_a.4.norm2_a.weight', 'audio_encoder.blocks_a.4.norm2_v.bias', 'audio_encoder.blocks_a.4.norm2_v.weight', 'audio_encoder.blocks_a.5.attn.proj.bias', 'audio_encoder.blocks_a.5.attn.proj.weight', 'audio_encoder.blocks_a.5.attn.qkv.bias', 'audio_encoder.blocks_a.5.attn.qkv.weight', 'audio_encoder.blocks_a.5.mlp.fc1.bias', 'audio_encoder.blocks_a.5.mlp.fc1.weight', 'audio_encoder.blocks_a.5.mlp.fc2.bias', 'audio_encoder.blocks_a.5.mlp.fc2.weight', 'audio_encoder.blocks_a.5.norm1.bias', 'audio_encoder.blocks_a.5.norm1.weight', 'audio_encoder.blocks_a.5.norm1_a.bias', 'audio_encoder.blocks_a.5.norm1_a.weight', 'audio_encoder.blocks_a.5.norm1_v.bias', 'audio_encoder.blocks_a.5.norm1_v.weight', 'audio_encoder.blocks_a.5.norm2.bias', 'audio_encoder.blocks_a.5.norm2.weight', 'audio_encoder.blocks_a.5.norm2_a.bias', 'audio_encoder.blocks_a.5.norm2_a.weight', 'audio_encoder.blocks_a.5.norm2_v.bias', 'audio_encoder.blocks_a.5.norm2_v.weight', 'audio_encoder.blocks_a.6.attn.proj.bias', 'audio_encoder.blocks_a.6.attn.proj.weight', 'audio_encoder.blocks_a.6.attn.qkv.bias', 'audio_encoder.blocks_a.6.attn.qkv.weight', 'audio_encoder.blocks_a.6.mlp.fc1.bias', 'audio_encoder.blocks_a.6.mlp.fc1.weight', 'audio_encoder.blocks_a.6.mlp.fc2.bias', 'audio_encoder.blocks_a.6.mlp.fc2.weight', 'audio_encoder.blocks_a.6.norm1.bias', 'audio_encoder.blocks_a.6.norm1.weight', 'audio_encoder.blocks_a.6.norm1_a.bias', 'audio_encoder.blocks_a.6.norm1_a.weight', 'audio_encoder.blocks_a.6.norm1_v.bias', 'audio_encoder.blocks_a.6.norm1_v.weight', 'audio_encoder.blocks_a.6.norm2.bias', 'audio_encoder.blocks_a.6.norm2.weight', 'audio_encoder.blocks_a.6.norm2_a.bias', 'audio_encoder.blocks_a.6.norm2_a.weight', 'audio_encoder.blocks_a.6.norm2_v.bias', 'audio_encoder.blocks_a.6.norm2_v.weight', 'audio_encoder.blocks_a.7.attn.proj.bias', 'audio_encoder.blocks_a.7.attn.proj.weight', 'audio_encoder.blocks_a.7.attn.qkv.bias', 'audio_encoder.blocks_a.7.attn.qkv.weight', 'audio_encoder.blocks_a.7.mlp.fc1.bias', 'audio_encoder.blocks_a.7.mlp.fc1.weight', 'audio_encoder.blocks_a.7.mlp.fc2.bias', 'audio_encoder.blocks_a.7.mlp.fc2.weight', 'audio_encoder.blocks_a.7.norm1.bias', 'audio_encoder.blocks_a.7.norm1.weight', 'audio_encoder.blocks_a.7.norm1_a.bias', 'audio_encoder.blocks_a.7.norm1_a.weight', 'audio_encoder.blocks_a.7.norm1_v.bias', 'audio_encoder.blocks_a.7.norm1_v.weight', 'audio_encoder.blocks_a.7.norm2.bias', 'audio_encoder.blocks_a.7.norm2.weight', 'audio_encoder.blocks_a.7.norm2_a.bias', 'audio_encoder.blocks_a.7.norm2_a.weight', 'audio_encoder.blocks_a.7.norm2_v.bias', 'audio_encoder.blocks_a.7.norm2_v.weight', 'audio_encoder.blocks_a.8.attn.proj.bias', 'audio_encoder.blocks_a.8.attn.proj.weight', 'audio_encoder.blocks_a.8.attn.qkv.bias', 'audio_encoder.blocks_a.8.attn.qkv.weight', 'audio_encoder.blocks_a.8.mlp.fc1.bias', 'audio_encoder.blocks_a.8.mlp.fc1.weight', 'audio_encoder.blocks_a.8.mlp.fc2.bias', 'audio_encoder.blocks_a.8.mlp.fc2.weight', 'audio_encoder.blocks_a.8.norm1.bias', 'audio_encoder.blocks_a.8.norm1.weight', 'audio_encoder.blocks_a.8.norm1_a.bias', 'audio_encoder.blocks_a.8.norm1_a.weight', 'audio_encoder.blocks_a.8.norm1_v.bias', 'audio_encoder.blocks_a.8.norm1_v.weight', 'audio_encoder.blocks_a.8.norm2.bias', 'audio_encoder.blocks_a.8.norm2.weight', 'audio_encoder.blocks_a.8.norm2_a.bias', 'audio_encoder.blocks_a.8.norm2_a.weight', 'audio_encoder.blocks_a.8.norm2_v.bias', 'audio_encoder.blocks_a.8.norm2_v.weight', 'audio_encoder.blocks_a.9.attn.proj.bias', 'audio_encoder.blocks_a.9.attn.proj.weight', 'audio_encoder.blocks_a.9.attn.qkv.bias', 'audio_encoder.blocks_a.9.attn.qkv.weight', 'audio_encoder.blocks_a.9.mlp.fc1.bias', 'audio_encoder.blocks_a.9.mlp.fc1.weight', 'audio_encoder.blocks_a.9.mlp.fc2.bias', 'audio_encoder.blocks_a.9.mlp.fc2.weight', 'audio_encoder.blocks_a.9.norm1.bias', 'audio_encoder.blocks_a.9.norm1.weight', 'audio_encoder.blocks_a.9.norm1_a.bias', 'audio_encoder.blocks_a.9.norm1_a.weight', 'audio_encoder.blocks_a.9.norm1_v.bias', 'audio_encoder.blocks_a.9.norm1_v.weight', 'audio_encoder.blocks_a.9.norm2.bias', 'audio_encoder.blocks_a.9.norm2.weight', 'audio_encoder.blocks_a.9.norm2_a.bias', 'audio_encoder.blocks_a.9.norm2_a.weight', 'audio_encoder.blocks_a.9.norm2_v.bias', 'audio_encoder.blocks_a.9.norm2_v.weight', 'audio_encoder.blocks_u.0.attn.proj.bias', 'audio_encoder.blocks_u.0.attn.proj.weight', 'audio_encoder.blocks_u.0.attn.qkv.bias', 'audio_encoder.blocks_u.0.attn.qkv.weight', 'audio_encoder.blocks_u.0.mlp.fc1.bias', 'audio_encoder.blocks_u.0.mlp.fc1.weight', 'audio_encoder.blocks_u.0.mlp.fc2.bias', 'audio_encoder.blocks_u.0.mlp.fc2.weight', 'audio_encoder.blocks_u.0.norm1.bias', 'audio_encoder.blocks_u.0.norm1.weight', 'audio_encoder.blocks_u.0.norm1_a.bias', 'audio_encoder.blocks_u.0.norm1_a.weight', 'audio_encoder.blocks_u.0.norm1_v.bias', 'audio_encoder.blocks_u.0.norm1_v.weight', 'audio_encoder.blocks_u.0.norm2.bias', 'audio_encoder.blocks_u.0.norm2.weight', 'audio_encoder.blocks_u.0.norm2_a.bias', 'audio_encoder.blocks_u.0.norm2_a.weight', 'audio_encoder.blocks_u.0.norm2_v.bias', 'audio_encoder.blocks_u.0.norm2_v.weight', 'audio_encoder.modality_a', 'audio_encoder.norm_a.bias', 'audio_encoder.norm_a.weight', 'audio_encoder.patch_embed_a.proj.bias', 'audio_encoder.patch_embed_a.proj.weight', 'audio_encoder.pos_embed_a', 'audio_proj.1.bias', 'audio_proj.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and config\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "config = T5Config.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# Initialize your custom model\n",
    "customT5 = CustomT5ForConditionalGeneration(config)\n",
    "model = customT5.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a941a9-fc97-48bc-9a1a-e51bdf931278",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.audio_encoder.initialize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cebe8f0d-b587-4471-b67f-3bfd5405b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.audio_proj[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949d95ef-f972-48b1-be03-7d67a4164f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def get_methods_and_params(cls):\n",
    "    methods_and_params = []\n",
    "    for name, member in inspect.getmembers(cls):\n",
    "        if inspect.ismethod(member) or inspect.isfunction(member):\n",
    "            parameters = inspect.signature(member).parameters\n",
    "            param_names = [param for param in parameters.keys() if param != 'self']\n",
    "            methods_and_params.append((name, tuple(param_names)))\n",
    "    return methods_and_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60da6ba5-44dc-437c-9282-a944b0536f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869658624"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf2146e1-efdc-48a7-a084-09b3d379b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./data/toy_dataset/openaqa_toy.json\"\n",
    "\n",
    "with open(file, \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c435d-eafe-4f64-9f28-6dd0e5c8f16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "116d9065-5f15-4d5a-8178-9dd6d3f4b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/toy_dataset/audio/4tnW9atZKo0.flac\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff4e5abb-b769-4452-aeb9-804142cc8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    path = data[i]['audio_id']\n",
    "    exten = path[len(path)-4:]\n",
    "    if exten == \"flac\" or exten == \".wav\":\n",
    "        mini_path = \"\"\n",
    "        for j in range(len(path)-1, -1, -1):\n",
    "            if path[j] == \"/\":\n",
    "                break\n",
    "            mini_path += path[j]\n",
    "        data[i]['audio_id'] = \"./data/toy_dataset/audio/\" + mini_path[::-1]\n",
    "        \n",
    "with open(file, \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67caa384-5ded-4ca9-a2bf-23e36042eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=file, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035c3b52-b557-4d9a-88b5-720125da5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80ebd258-ca28-4e60-8659-e1d12fd45e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5044, 6), (1262, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape, test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "039bef0a-76ac-412a-9ee3-31bc437727d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is the relationship between the onset of the shuffle sound and the music?',\n",
       " 'input': '',\n",
       " 'audio_id': './data/toy_dataset/audio/z-cmn8J7uWw.flac',\n",
       " 'dataset': 'as_strong_train',\n",
       " 'task': 'open-ended question',\n",
       " 'output': 'The onset of the shuffle sound is not synchronized with the onset of the music. They appear at different time stamps.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c4c270-b7fc-4f13-a362-84f0e23ee5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filterbank\n",
    "def load_audio(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    audio_info = 'Original input audio length {:.2f} seconds, number of channels: {:d}, sampling rate: {:d}.'.format(waveform.shape[1]/sample_rate, waveform.shape[0], sample_rate)\n",
    "    if waveform.shape[0] != 1:\n",
    "        waveform = waveform[0].unsqueeze(0)\n",
    "        audio_info += ' Only the first channel is used.'\n",
    "    if sample_rate == 16000:\n",
    "        pass\n",
    "    else:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=sample_rate, new_freq=16000)\n",
    "        sample_rate = 16000\n",
    "        audio_info += ' Resample to 16000Hz.'\n",
    "    waveform = waveform - waveform.mean()\n",
    "    fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sample_rate,\n",
    "                                              use_energy=False, window_type='hanning',\n",
    "                                              num_mel_bins=128, dither=0.0, frame_shift=10)\n",
    "    target_length = 1024\n",
    "    n_frames = fbank.shape[0]\n",
    "    p = target_length - n_frames\n",
    "    if p > 0:\n",
    "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "        fbank = m(fbank)\n",
    "    elif p < 0:\n",
    "        fbank = fbank[0:target_length, :]\n",
    "    # normalize the fbank\n",
    "    fbank = (fbank + 5.081) / 4.4849\n",
    "    return fbank, audio_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "726a253e-ba72-41a3-94d3-e46f95459c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def return_audio(path):\n",
    "\n",
    "    cur_audio_input, audio_info = load_audio(path)\n",
    "    cur_audio_input = cur_audio_input.unsqueeze(0)\n",
    "    \n",
    "    # projecting to 1024 input embedding dimension for T5\n",
    "    audio_proj = nn.Sequential(nn.LayerNorm(768, elementwise_affine=False), nn.Linear(768, 1024))\n",
    "    audio_input = audio_encoder(cur_audio_input)  # [B, 512, 768]\n",
    "    audio_input = audio_input.reshape(audio_input.shape[0], 8, 64, audio_input.shape[-1])\n",
    "    audio_input = torch.mean(audio_input, dim=1)  # mean pool over the frequency dimension # [B, 64, 768]\n",
    "    audio_input = torch.nn.functional.avg_pool2d(audio_input, (2, 1)) #[B, 32, 768]\n",
    "    # hard norm to 50\n",
    "    audio_input = audio_input / 50\n",
    "    audio_input = audio_proj(audio_input) #[B, 32, 1024]\n",
    "    \n",
    "    return audio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c664636-70a2-44d0-9451-b6a8c321ecf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abfcde6e-8a70-4175-8b7e-3fe3c7149121",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42670ff5-a524-4846-9f6b-c93b944ed181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CombinedEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, model, device, max_length=512):\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instruction = self.data[idx]['instruction']\n",
    "        audio_path = self.data[idx]['audio_id']\n",
    "        label = self.data[idx]['output']\n",
    "        \n",
    "        input_ids = self.tokenizer(instruction, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        decoder_input_ids = self.tokenizer(label, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        audio_bank, audio_info = load_audio(audio_path)\n",
    "        audio_bank = audio_bank.to(self.device)\n",
    "        \n",
    "        return input_ids, decoder_input_ids.squeeze(0), audio_bank\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    input_ids = [item[0].squeeze(0) for item in batch]  # Remove unnecessary dim\n",
    "    decoder_input_ids = [item[1] for item in batch]\n",
    "    audio_input = [item[2] for item in batch]\n",
    "\n",
    "    # Pad input_ids and decoder_input_ids to the maximum length in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    decoder_input_ids_padded = pad_sequence(decoder_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Stack audio inputs directly if they have the same size\n",
    "    audio_input_stacked = torch.stack(audio_input)\n",
    "    return input_ids_padded, audio_input_stacked, decoder_input_ids_padded\n",
    "\n",
    "\n",
    "dataset = CombinedEmbeddingsDataset(train_dataset, tokenizer, model, device)\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d6e7478-a346-430f-b431-1ad6ece42a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.10282817482948303\n",
      "loss: 0.09804828464984894\n",
      "loss: 0.09341374039649963\n",
      "loss: 0.0889216959476471\n",
      "loss: 0.08478978276252747\n",
      "loss: 0.08118700236082077\n",
      "loss: 0.07802993804216385\n",
      "loss: 0.07519853860139847\n",
      "loss: 0.07254151254892349\n",
      "loss: 0.06993664801120758\n"
     ]
    }
   ],
   "source": [
    "# Example labels (target sequence for training)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    loss = 0\n",
    "    # Iterate through the DataLoader\n",
    "    for i, (input_ids, audio_bank, decoder_input_ids) in enumerate(itertools.islice(dataloader, 64)):\n",
    "        if torch.isnan(input_ids).any() or torch.isnan(audio_bank).any() or torch.isnan(decoder_input_ids).any():\n",
    "            print(\"NaN detected in input tensors\")\n",
    "\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, audio_input=audio_bank,labels=decoder_input_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = outputs.loss\n",
    "        #print(\"loss:\", loss.item())\n",
    "        loss.backward()\n",
    "        loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"loss:\",(loss/64).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70b4ebe8-5573-455b-8fe1-c888b007921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, \"working_model_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c34993b-8ea6-4622-8894-f5d937fb29e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"./data/toy_dataset/audio/4xyVFghUEPc.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0).to(device)\n",
    "prompt_text = \"classify audio:\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=input_ids, audio_input=cur_audio_input)\n",
    "    \n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646d306-1ba5-4967-8e3d-91d77328ad34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cfd7a6-78ed-4d1d-b4ea-52115a298760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6669f4d-2425-4258-b079-eb694655e45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5739953-8610-42b5-8bc6-8c046822d6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014ed27-f182-4946-b8cf-74a71f96de60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225acc9-ea9c-4665-a7fa-5891338c9b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3b903-748b-4cfb-851c-bacff8a599bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56207f0-2a07-4b0a-8973-76d3f6490f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db39cfaa-5833-4bea-8b7d-242fddd2ff65",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61e18f-41af-4dcd-8fed-676f33666e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"working_model_2.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10efe568-3e31-4845-af27-f422fd7e0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cave_model import CAVMAEFTAudio\n",
    "audio_encoder = CAVMAEFTAudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c1a5635-50ab-4e71-8fb5-4b2c1355ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from CAVE inside: tensor([[[-2.2843, -2.4218, -2.0455,  ..., -0.9920, -1.5722, -2.3284],\n",
      "         [-1.8898, -2.3895, -1.6217,  ..., -0.2275, -1.0407, -2.1476],\n",
      "         [-1.6393, -2.1380, -1.3702,  ...,  0.0030, -0.7107, -1.5814],\n",
      "         ...,\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329]]])\n",
      "shape before CAVE: torch.Size([1, 1024, 128])\n",
      "from CAVE before patch: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]])\n",
      "patch embed: torch.Size([1, 1, 128, 1024])\n",
      "before convo: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]])\n",
      "from CAVE after patch: tensor([[[ 2.6611, -1.2217, -0.2890,  ..., -0.4917, -0.9088,  0.5112],\n",
      "         [ 2.8197, -1.7707, -0.3036,  ..., -0.0717, -1.1140,  0.7702],\n",
      "         [ 3.2779, -2.0465, -0.2818,  ..., -0.2168, -1.5610,  1.0363],\n",
      "         ...,\n",
      "         [-1.7904,  1.0632,  0.1043,  ...,  0.2845,  0.6632, -0.7552],\n",
      "         [-1.7904,  1.0632,  0.1043,  ...,  0.2845,  0.6632, -0.7552],\n",
      "         [-1.7904,  1.0632,  0.1043,  ...,  0.2845,  0.6632, -0.7552]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "from CAVE inside 2: tensor([[[ 2.6773, -1.2612, -0.2841,  ...,  0.4644,  0.1219,  1.4585],\n",
      "         [ 3.6774, -0.9949,  0.4899,  ...,  0.8844, -0.0833,  1.7175],\n",
      "         [ 4.2034, -1.1418,  0.6930,  ...,  0.7394, -0.5303,  1.9836],\n",
      "         ...,\n",
      "         [-2.7403,  2.0234, -0.7949,  ...,  1.2407,  1.6939,  0.1920],\n",
      "         [-2.5134,  1.5836, -0.1096,  ...,  1.2407,  1.6939,  0.1920],\n",
      "         [-1.6068,  0.6724,  0.7441,  ...,  1.2407,  1.6939,  0.1920]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "shape before putting in block: torch.Size([1, 512, 768])\n",
      "from CAVE inside 3: tensor([[[-1.9227, -0.8392, -0.0687,  ..., -0.0199, -0.1403, -0.5158],\n",
      "         [-1.8927, -0.8580, -0.0356,  ...,  0.0313, -0.1316, -0.5077],\n",
      "         [-1.8534, -0.8358,  0.0105,  ...,  0.0134, -0.0942, -0.5143],\n",
      "         ...,\n",
      "         [-2.3141, -0.5856, -0.3207,  ..., -0.3211,  0.0934, -0.4715],\n",
      "         [-2.3242, -0.6239, -0.3015,  ..., -0.2697,  0.0892, -0.4805],\n",
      "         [-2.3116, -0.6845, -0.2764,  ..., -0.2048,  0.0723, -0.4951]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "from CAVE inside 4: tensor([[[-1.7023, -0.7344, -0.0461,  ..., -0.0025, -0.1100, -0.4455],\n",
      "         [-1.6768, -0.7517, -0.0163,  ...,  0.0435, -0.1022, -0.4384],\n",
      "         [-1.6403, -0.7311,  0.0251,  ...,  0.0277, -0.0684, -0.4439],\n",
      "         ...,\n",
      "         [-2.0549, -0.5034, -0.2656,  ..., -0.2660,  0.1061, -0.4010],\n",
      "         [-2.0651, -0.5383, -0.2489,  ..., -0.2203,  0.1020, -0.4096],\n",
      "         [-2.0548, -0.5933, -0.2267,  ..., -0.1624,  0.0865, -0.4231]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7023, -0.7344, -0.0461,  ..., -0.0025, -0.1100, -0.4455],\n",
       "         [-1.6768, -0.7517, -0.0163,  ...,  0.0435, -0.1022, -0.4384],\n",
       "         [-1.6403, -0.7311,  0.0251,  ...,  0.0277, -0.0684, -0.4439],\n",
       "         ...,\n",
       "         [-2.0549, -0.5034, -0.2656,  ..., -0.2660,  0.1061, -0.4010],\n",
       "         [-2.0651, -0.5383, -0.2489,  ..., -0.2203,  0.1020, -0.4096],\n",
       "         [-2.0548, -0.5933, -0.2267,  ..., -0.1624,  0.0865, -0.4231]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"./data/toy_dataset/audio/577026.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0)\n",
    "audio_input = audio_encoder(cur_audio_input) \n",
    "audio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4241216e-6a36-4ead-b4c7-1b3f7fc434f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from CAVE inside: tensor([[[-2.2843, -2.4218, -2.0455,  ..., -0.9920, -1.5722, -2.3284],\n",
      "         [-1.8898, -2.3895, -1.6217,  ..., -0.2275, -1.0407, -2.1476],\n",
      "         [-1.6393, -2.1380, -1.3702,  ...,  0.0030, -0.7107, -1.5814],\n",
      "         ...,\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329]]],\n",
      "       device='cuda:0')\n",
      "shape before CAVE: torch.Size([1, 1024, 128])\n",
      "from CAVE before patch: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]],\n",
      "       device='cuda:0')\n",
      "patch embed: torch.Size([1, 1, 128, 1024])\n",
      "before convo: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]],\n",
      "       device='cuda:0')\n",
      "from CAVE after patch: tensor([[[-1.9091, -0.8451,  0.2472,  ...,  0.1447, -0.0661,  0.8762],\n",
      "         [-2.0660, -1.2898,  0.4945,  ...,  0.3127, -1.1853,  1.2340],\n",
      "         [-2.3570, -1.1388,  0.7489,  ...,  0.3829, -1.1931,  0.9730],\n",
      "         ...,\n",
      "         [ 0.7995,  0.6390, -0.5792,  ..., -0.1193,  0.6473, -0.6660],\n",
      "         [ 0.7995,  0.6390, -0.5792,  ..., -0.1193,  0.6473, -0.6660],\n",
      "         [ 0.7995,  0.6390, -0.5792,  ..., -0.1193,  0.6473, -0.6660]]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "from CAVE inside 2: tensor([[[-1.9106, -0.8678,  0.2206,  ...,  1.1831,  0.9352,  1.8581],\n",
      "         [-1.2259, -0.4973,  1.2565,  ...,  1.3511, -0.1839,  2.2159],\n",
      "         [-1.4492, -0.2173,  1.6922,  ...,  1.4213, -0.1917,  1.9549],\n",
      "         ...,\n",
      "         [-0.1680,  1.6160, -1.5098,  ...,  0.9191,  1.6487,  0.3159],\n",
      "         [ 0.0589,  1.1762, -0.8246,  ...,  0.9191,  1.6487,  0.3159],\n",
      "         [ 0.9654,  0.2650,  0.0291,  ...,  0.9191,  1.6487,  0.3159]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "shape before putting in block: torch.Size([1, 512, 768])\n",
      "from CAVE inside 3: tensor([[[ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262],\n",
      "         [ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262],\n",
      "         [ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262],\n",
      "         ...,\n",
      "         [ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262],\n",
      "         [ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262],\n",
      "         [ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "from CAVE inside 4: tensor([[[ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739],\n",
      "         [ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739],\n",
      "         [ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739],\n",
      "         ...,\n",
      "         [ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739],\n",
      "         [ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739],\n",
      "         [ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "Audio: tensor([[[5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41],\n",
      "         [5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41],\n",
      "         [5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41],\n",
      "         ...,\n",
      "         [5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41],\n",
      "         [5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41],\n",
      "         [5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "input_embeds: tensor([[[ 7.5833, -5.9968, -8.7719,  ..., -1.7548, -1.4999,  0.8449],\n",
      "         [ 8.0296,  3.5735, -0.2749,  ..., -0.3696,  0.0123, -2.4261],\n",
      "         [-7.9453, -3.8064,  5.7910,  ...,  2.7917,  1.0186,  1.6824],\n",
      "         ...,\n",
      "         [-7.1989, -7.0903, 10.7553,  ...,  7.1836,  5.0034, -9.7338],\n",
      "         [ 1.5147,  3.0822, -0.0883,  ...,  0.4944,  3.2871, -6.7017],\n",
      "         [-2.7021, -2.1235,  0.5805,  ..., -2.3538, -4.8025, -5.8202]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "concat shape: torch.Size([1, 43, 1024])\n"
     ]
    }
   ],
   "source": [
    "file = \"./data/toy_dataset/audio/577026.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0).to(device)\n",
    "prompt_text = \"what can be infered from this audio following\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "target_text = \"It can be inferred that the audio is a recording of a musical performance or a rehearsal.The combination of music, speech,\"\\\n",
    "\"and sound effects suggests that the audio is a representation of a musical performance or a rehearsal, where the music is being played\"\\\n",
    "\"and the performers are practicing their performance or rehearsing.\"\n",
    "target_ids = tokenizer(target_text, return_tensors='pt').input_ids.to(device)\n",
    "decoder_input_ids = model._shift_right(target_ids).to(device)\n",
    "\n",
    "outputs = model(input_ids=input_ids, audio_input=cur_audio_input,labels=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "657ae9e4-a045-46be-9513-55aa19c489ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There\n"
     ]
    }
   ],
   "source": [
    "file = \"./data/toy_dataset/audio/3CHor3uzS00.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0).to(device)\n",
    "prompt_text = \"what can be infered from this audio following\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "target_text = \"\"\n",
    "target_ids = tokenizer(target_text, return_tensors='pt').input_ids.to(device)\n",
    "decoder_input_ids = model._shift_right(target_ids).to(device)\n",
    "\n",
    "outputs = model(input_ids=input_ids, audio_input=cur_audio_input,labels=decoder_input_ids)\n",
    "logits = outputs.logits  \n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "decoded_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429a4ed-867c-4a2c-8cb3-22beb5a44985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "audio_encoder = CAVMAEFTAudio()\n",
    "prompt_text = \"what can be infered from this audio following\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "audio_input = return_audio(\"./data/toy_dataset/audio/_4X8RNeWeDI.flac\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_embeddings = model.shared(input_ids)  # Shape: (1, sequence_length, 1024)\n",
    "    prompt_embeddings = prompt_embeddings.to(device)\n",
    "\n",
    "\n",
    "target_text = \"It can be inferred that the audio is a recording of a musical performance or a rehearsal.The combination of music, speech,\"\\\n",
    "\"and sound effects suggests that the audio is a representation of a musical performance or a rehearsal, where the music is being played\"\\\n",
    "\"and the performers are practicing their performance or rehearsing.\"\n",
    "\n",
    "target_ids = tokenizer(target_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "decoder_input_ids = model._shift_right(target_ids)\n",
    "\n",
    "audio_embeddings = audio_input.to(device)  # Shape: (1, 32, 1024)\n",
    "\n",
    "# Concatenate prompt and audio embeddings\n",
    "combined_embeddings = torch.cat((prompt_embeddings, audio_embeddings), dim=1)  # Shape: (1, sequence_length + 32, 1024)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "\n",
    "if combined_embeddings.size(1) > max_length:\n",
    "    combined_embeddings = combined_embeddings[:, :max_length, :]\n",
    "\n",
    "padding_length = max_length - combined_embeddings.size(1)\n",
    "if padding_length > 0:\n",
    "    padding_tensor = torch.zeros((combined_embeddings.size(0), padding_length, combined_embeddings.size(2))).to(device)\n",
    "    combined_embeddings = torch.cat((combined_embeddings, padding_tensor), dim=1)\n",
    "\n",
    "\n",
    "attention_mask = torch.ones(combined_embeddings.size(0), combined_embeddings.size(1)).to(device)\n",
    "if padding_length > 0:\n",
    "    attention_mask[:, -padding_length:] = 0\n",
    "\n",
    "\n",
    "outputs = model.generate(inputs_embeds=combined_embeddings, attention_mask=attention_mask,labels=decoder_input_ids)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623bfc9b-d511-404c-a4a3-3170a3991509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
