{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d904a05b-9a7d-4e55-b5e8-d94f69d0020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "import torchaudio\n",
    "from Cave_model import CAVMAEFTAudio\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "import torch.nn.init as init\n",
    "from transformers.modeling_outputs import BaseModelOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8edd18d-dc26-4abe-bb6e-9dcaef4e75bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sayyss/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b696eab-249f-4adc-90e0-b1f92ae530fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb49928-847b-40a5-904e-3ef0234e2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3343f990-9a17-4126-8fd7-5a0d0eb041a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomT5ForConditionalGeneration(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.audio_encoder = CAVMAEFTAudio()\n",
    "        self.audio_proj = nn.Sequential(nn.LayerNorm(768, elementwise_affine=False), nn.Linear(768, 1024))\n",
    "\n",
    "        #nn.init.constant_(self.audio_proj[0].weight, 1)\n",
    "        #nn.init.constant_(self.audio_proj[0].bias, 0)\n",
    "        #nn.init.kaiming_normal_(self.audio_proj[1].weight, nonlinearity='relu')\n",
    "        #if self.audio_proj[1].bias is not None:\n",
    "        #    nn.init.constant_(self.audio_proj[1].bias, 0)\n",
    "        \"\"\"                                                                                       \n",
    "        self.audio_encoder.initialize_weights()\n",
    "        init.kaiming_normal_(self.audio_encoder.patch_embed_a.proj.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if self.audio_encoder.patch_embed_a.proj.bias is not None:\n",
    "            init.constant_(self.audio_encoder.patch_embed_a.proj.bias, 0)\n",
    "            \"\"\"\n",
    "        self.post_init()\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        audio_input,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        decoder_attention_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # cut decoder_input_ids if past_key_values is used\n",
    "        if past_key_values is not None:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "\n",
    "            # Some generation methods already pass only the last input ID\n",
    "            if input_ids.shape[1] > past_length:\n",
    "                remove_prefix_length = past_length\n",
    "            else:\n",
    "                # Default to old behavior: keep only final ID\n",
    "                remove_prefix_length = input_ids.shape[1] - 1\n",
    "\n",
    "            input_ids = input_ids[:, remove_prefix_length:]\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"decoder_input_ids\": input_ids,\n",
    "            \"audio_input\": audio_input,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"decoder_attention_mask\": decoder_attention_mask,\n",
    "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        audio_input = None,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n",
    "            config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n",
    "            labels in `[0, ..., config.vocab_size]`\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "        >>> model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "        >>> # training\n",
    "        >>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "        >>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "        >>> outputs = model(input_ids=input_ids, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\n",
    "        >>> # inference\n",
    "        >>> input_ids = tokenizer(\n",
    "        ...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    "        ... ).input_ids  # Batch size 1\n",
    "        >>> outputs = model.generate(input_ids)\n",
    "        >>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        >>> # studies have shown that owning a dog is good for you.\n",
    "        ```\"\"\"\n",
    "        \n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
    "        if head_mask is not None and decoder_head_mask is None:\n",
    "            if self.config.num_layers == self.config.num_decoder_layers:\n",
    "                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n",
    "                decoder_head_mask = head_mask\n",
    "\n",
    "        # ******** Custom modifications start *********\n",
    "        # Get audio embeddings\n",
    "        if audio_input == None:\n",
    "            raise ValueError(\"audio input cannot be empty\")\n",
    "            \n",
    "        audio_input = self.audio_encoder(audio_input)  # [B, 512, 768]\n",
    "        audio_input = audio_input.reshape(audio_input.shape[0], 8, 64, audio_input.shape[-1])\n",
    "        audio_input = torch.mean(audio_input, dim=1)  # mean pool over the frequency dimension # [B, 64, 768]\n",
    "        audio_input = torch.nn.functional.avg_pool2d(audio_input, (2, 1)) #[B, 32, 768\n",
    "        # hard norm to 50\n",
    "        audio_input = audio_input / 50\n",
    "        audio_input = self.audio_proj(audio_input) #[B, 32, 1024]\n",
    "        audio_length = audio_input.shape[1]\n",
    "        \n",
    "        # Custom: get embeddings \n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.shared(input_ids).to(device)\n",
    "\n",
    "        #print(\"Audio:\", audio_input)\n",
    "        #print(\"input_embeds:\", inputs_embeds)\n",
    "        seq_length = audio_length + inputs_embeds.shape[1] # [32+seq_length]\n",
    "    \n",
    "        audio_embeds = audio_input.to(device) # [2,32,1024] \n",
    "        #print(audio_embeds.shape)\n",
    "        #print(inputs_embeds.shape) # shape: [2,seq_length, 1024]\n",
    "        inputs_embeds = torch.cat((inputs_embeds, audio_embeds), dim=1)  # Shape: (2,sequence_length + 32, 1024)\n",
    "        #print(\"concat shape:\",inputs_embeds.shape) # shape: [2,seq+32, 1024]\n",
    "\n",
    "        max_length = 512\n",
    "        seq_length = inputs_embeds.size(1)  \n",
    "        padding_length = max_length - seq_length\n",
    "        \n",
    "        # Truncate if the sequence length exceeds max_length\n",
    "        if seq_length > max_length:\n",
    "            inputs_embeds = inputs_embeds[:, :max_length, :]\n",
    "        \n",
    "        # Apply padding if the sequence is shorter than max_length\n",
    "        if padding_length > 0:\n",
    "            padding_tensor = torch.zeros((inputs_embeds.size(0), padding_length, inputs_embeds.size(2))).to(device)\n",
    "            inputs_embeds = torch.cat((inputs_embeds, padding_tensor), dim=1) \n",
    "        \n",
    "\n",
    "        attention_mask = torch.ones((inputs_embeds.size(0), inputs_embeds.size(1))).to(device)  # Match sequence dimension\n",
    "\n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        if encoder_outputs is None:\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=None, # Custom: change to none because we already defined embeddings\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        hidden_states = encoder_outputs[0]\n",
    "\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "\n",
    "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            # get decoder inputs from shifting lm labels to the right\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "            hidden_states = hidden_states.to(self.decoder.first_device)\n",
    "            if decoder_input_ids is not None:\n",
    "                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.decoder.first_device)\n",
    "            if decoder_attention_mask is not None:\n",
    "                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
    "\n",
    "        \"\"\"\n",
    "        # Generate decoder_attention_mask\n",
    "        if decoder_input_ids is not None:\n",
    "            decoder_seq_length = decoder_input_ids.shape[1]\n",
    "            decoder_attention_mask = torch.triu(\n",
    "                torch.ones((decoder_seq_length, decoder_seq_length), dtype=torch.bool, device=decoder_input_ids.device),\n",
    "                diagonal=1\n",
    "            )\n",
    "        else:\n",
    "            decoder_attention_mask = None\n",
    "\"\"\"\n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = decoder_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.encoder.first_device)\n",
    "            self.lm_head = self.lm_head.to(self.encoder.first_device)\n",
    "            sequence_output = sequence_output.to(self.lm_head.weight.device)\n",
    "\n",
    "        if self.config.tie_word_embeddings:\n",
    "            # Rescale output before projecting on vocab\n",
    "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "            sequence_output = sequence_output * (self.model_dim**-0.5)\n",
    "\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            # move labels to correct device to enable PP\n",
    "            labels = labels.to(lm_logits.device)\n",
    "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e8489-edf1-4f6b-83e2-314e004d6f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f59470-b3b8-4e11-87e5-2c5291e33ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/sayyss/.conda/envs/LTU-Replication/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of CustomT5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['audio_encoder.blocks_a.0.attn.proj.bias', 'audio_encoder.blocks_a.0.attn.proj.weight', 'audio_encoder.blocks_a.0.attn.qkv.bias', 'audio_encoder.blocks_a.0.attn.qkv.weight', 'audio_encoder.blocks_a.0.mlp.fc1.bias', 'audio_encoder.blocks_a.0.mlp.fc1.weight', 'audio_encoder.blocks_a.0.mlp.fc2.bias', 'audio_encoder.blocks_a.0.mlp.fc2.weight', 'audio_encoder.blocks_a.0.norm1.bias', 'audio_encoder.blocks_a.0.norm1.weight', 'audio_encoder.blocks_a.0.norm1_a.bias', 'audio_encoder.blocks_a.0.norm1_a.weight', 'audio_encoder.blocks_a.0.norm1_v.bias', 'audio_encoder.blocks_a.0.norm1_v.weight', 'audio_encoder.blocks_a.0.norm2.bias', 'audio_encoder.blocks_a.0.norm2.weight', 'audio_encoder.blocks_a.0.norm2_a.bias', 'audio_encoder.blocks_a.0.norm2_a.weight', 'audio_encoder.blocks_a.0.norm2_v.bias', 'audio_encoder.blocks_a.0.norm2_v.weight', 'audio_encoder.blocks_a.1.attn.proj.bias', 'audio_encoder.blocks_a.1.attn.proj.weight', 'audio_encoder.blocks_a.1.attn.qkv.bias', 'audio_encoder.blocks_a.1.attn.qkv.weight', 'audio_encoder.blocks_a.1.mlp.fc1.bias', 'audio_encoder.blocks_a.1.mlp.fc1.weight', 'audio_encoder.blocks_a.1.mlp.fc2.bias', 'audio_encoder.blocks_a.1.mlp.fc2.weight', 'audio_encoder.blocks_a.1.norm1.bias', 'audio_encoder.blocks_a.1.norm1.weight', 'audio_encoder.blocks_a.1.norm1_a.bias', 'audio_encoder.blocks_a.1.norm1_a.weight', 'audio_encoder.blocks_a.1.norm1_v.bias', 'audio_encoder.blocks_a.1.norm1_v.weight', 'audio_encoder.blocks_a.1.norm2.bias', 'audio_encoder.blocks_a.1.norm2.weight', 'audio_encoder.blocks_a.1.norm2_a.bias', 'audio_encoder.blocks_a.1.norm2_a.weight', 'audio_encoder.blocks_a.1.norm2_v.bias', 'audio_encoder.blocks_a.1.norm2_v.weight', 'audio_encoder.blocks_a.10.attn.proj.bias', 'audio_encoder.blocks_a.10.attn.proj.weight', 'audio_encoder.blocks_a.10.attn.qkv.bias', 'audio_encoder.blocks_a.10.attn.qkv.weight', 'audio_encoder.blocks_a.10.mlp.fc1.bias', 'audio_encoder.blocks_a.10.mlp.fc1.weight', 'audio_encoder.blocks_a.10.mlp.fc2.bias', 'audio_encoder.blocks_a.10.mlp.fc2.weight', 'audio_encoder.blocks_a.10.norm1.bias', 'audio_encoder.blocks_a.10.norm1.weight', 'audio_encoder.blocks_a.10.norm1_a.bias', 'audio_encoder.blocks_a.10.norm1_a.weight', 'audio_encoder.blocks_a.10.norm1_v.bias', 'audio_encoder.blocks_a.10.norm1_v.weight', 'audio_encoder.blocks_a.10.norm2.bias', 'audio_encoder.blocks_a.10.norm2.weight', 'audio_encoder.blocks_a.10.norm2_a.bias', 'audio_encoder.blocks_a.10.norm2_a.weight', 'audio_encoder.blocks_a.10.norm2_v.bias', 'audio_encoder.blocks_a.10.norm2_v.weight', 'audio_encoder.blocks_a.2.attn.proj.bias', 'audio_encoder.blocks_a.2.attn.proj.weight', 'audio_encoder.blocks_a.2.attn.qkv.bias', 'audio_encoder.blocks_a.2.attn.qkv.weight', 'audio_encoder.blocks_a.2.mlp.fc1.bias', 'audio_encoder.blocks_a.2.mlp.fc1.weight', 'audio_encoder.blocks_a.2.mlp.fc2.bias', 'audio_encoder.blocks_a.2.mlp.fc2.weight', 'audio_encoder.blocks_a.2.norm1.bias', 'audio_encoder.blocks_a.2.norm1.weight', 'audio_encoder.blocks_a.2.norm1_a.bias', 'audio_encoder.blocks_a.2.norm1_a.weight', 'audio_encoder.blocks_a.2.norm1_v.bias', 'audio_encoder.blocks_a.2.norm1_v.weight', 'audio_encoder.blocks_a.2.norm2.bias', 'audio_encoder.blocks_a.2.norm2.weight', 'audio_encoder.blocks_a.2.norm2_a.bias', 'audio_encoder.blocks_a.2.norm2_a.weight', 'audio_encoder.blocks_a.2.norm2_v.bias', 'audio_encoder.blocks_a.2.norm2_v.weight', 'audio_encoder.blocks_a.3.attn.proj.bias', 'audio_encoder.blocks_a.3.attn.proj.weight', 'audio_encoder.blocks_a.3.attn.qkv.bias', 'audio_encoder.blocks_a.3.attn.qkv.weight', 'audio_encoder.blocks_a.3.mlp.fc1.bias', 'audio_encoder.blocks_a.3.mlp.fc1.weight', 'audio_encoder.blocks_a.3.mlp.fc2.bias', 'audio_encoder.blocks_a.3.mlp.fc2.weight', 'audio_encoder.blocks_a.3.norm1.bias', 'audio_encoder.blocks_a.3.norm1.weight', 'audio_encoder.blocks_a.3.norm1_a.bias', 'audio_encoder.blocks_a.3.norm1_a.weight', 'audio_encoder.blocks_a.3.norm1_v.bias', 'audio_encoder.blocks_a.3.norm1_v.weight', 'audio_encoder.blocks_a.3.norm2.bias', 'audio_encoder.blocks_a.3.norm2.weight', 'audio_encoder.blocks_a.3.norm2_a.bias', 'audio_encoder.blocks_a.3.norm2_a.weight', 'audio_encoder.blocks_a.3.norm2_v.bias', 'audio_encoder.blocks_a.3.norm2_v.weight', 'audio_encoder.blocks_a.4.attn.proj.bias', 'audio_encoder.blocks_a.4.attn.proj.weight', 'audio_encoder.blocks_a.4.attn.qkv.bias', 'audio_encoder.blocks_a.4.attn.qkv.weight', 'audio_encoder.blocks_a.4.mlp.fc1.bias', 'audio_encoder.blocks_a.4.mlp.fc1.weight', 'audio_encoder.blocks_a.4.mlp.fc2.bias', 'audio_encoder.blocks_a.4.mlp.fc2.weight', 'audio_encoder.blocks_a.4.norm1.bias', 'audio_encoder.blocks_a.4.norm1.weight', 'audio_encoder.blocks_a.4.norm1_a.bias', 'audio_encoder.blocks_a.4.norm1_a.weight', 'audio_encoder.blocks_a.4.norm1_v.bias', 'audio_encoder.blocks_a.4.norm1_v.weight', 'audio_encoder.blocks_a.4.norm2.bias', 'audio_encoder.blocks_a.4.norm2.weight', 'audio_encoder.blocks_a.4.norm2_a.bias', 'audio_encoder.blocks_a.4.norm2_a.weight', 'audio_encoder.blocks_a.4.norm2_v.bias', 'audio_encoder.blocks_a.4.norm2_v.weight', 'audio_encoder.blocks_a.5.attn.proj.bias', 'audio_encoder.blocks_a.5.attn.proj.weight', 'audio_encoder.blocks_a.5.attn.qkv.bias', 'audio_encoder.blocks_a.5.attn.qkv.weight', 'audio_encoder.blocks_a.5.mlp.fc1.bias', 'audio_encoder.blocks_a.5.mlp.fc1.weight', 'audio_encoder.blocks_a.5.mlp.fc2.bias', 'audio_encoder.blocks_a.5.mlp.fc2.weight', 'audio_encoder.blocks_a.5.norm1.bias', 'audio_encoder.blocks_a.5.norm1.weight', 'audio_encoder.blocks_a.5.norm1_a.bias', 'audio_encoder.blocks_a.5.norm1_a.weight', 'audio_encoder.blocks_a.5.norm1_v.bias', 'audio_encoder.blocks_a.5.norm1_v.weight', 'audio_encoder.blocks_a.5.norm2.bias', 'audio_encoder.blocks_a.5.norm2.weight', 'audio_encoder.blocks_a.5.norm2_a.bias', 'audio_encoder.blocks_a.5.norm2_a.weight', 'audio_encoder.blocks_a.5.norm2_v.bias', 'audio_encoder.blocks_a.5.norm2_v.weight', 'audio_encoder.blocks_a.6.attn.proj.bias', 'audio_encoder.blocks_a.6.attn.proj.weight', 'audio_encoder.blocks_a.6.attn.qkv.bias', 'audio_encoder.blocks_a.6.attn.qkv.weight', 'audio_encoder.blocks_a.6.mlp.fc1.bias', 'audio_encoder.blocks_a.6.mlp.fc1.weight', 'audio_encoder.blocks_a.6.mlp.fc2.bias', 'audio_encoder.blocks_a.6.mlp.fc2.weight', 'audio_encoder.blocks_a.6.norm1.bias', 'audio_encoder.blocks_a.6.norm1.weight', 'audio_encoder.blocks_a.6.norm1_a.bias', 'audio_encoder.blocks_a.6.norm1_a.weight', 'audio_encoder.blocks_a.6.norm1_v.bias', 'audio_encoder.blocks_a.6.norm1_v.weight', 'audio_encoder.blocks_a.6.norm2.bias', 'audio_encoder.blocks_a.6.norm2.weight', 'audio_encoder.blocks_a.6.norm2_a.bias', 'audio_encoder.blocks_a.6.norm2_a.weight', 'audio_encoder.blocks_a.6.norm2_v.bias', 'audio_encoder.blocks_a.6.norm2_v.weight', 'audio_encoder.blocks_a.7.attn.proj.bias', 'audio_encoder.blocks_a.7.attn.proj.weight', 'audio_encoder.blocks_a.7.attn.qkv.bias', 'audio_encoder.blocks_a.7.attn.qkv.weight', 'audio_encoder.blocks_a.7.mlp.fc1.bias', 'audio_encoder.blocks_a.7.mlp.fc1.weight', 'audio_encoder.blocks_a.7.mlp.fc2.bias', 'audio_encoder.blocks_a.7.mlp.fc2.weight', 'audio_encoder.blocks_a.7.norm1.bias', 'audio_encoder.blocks_a.7.norm1.weight', 'audio_encoder.blocks_a.7.norm1_a.bias', 'audio_encoder.blocks_a.7.norm1_a.weight', 'audio_encoder.blocks_a.7.norm1_v.bias', 'audio_encoder.blocks_a.7.norm1_v.weight', 'audio_encoder.blocks_a.7.norm2.bias', 'audio_encoder.blocks_a.7.norm2.weight', 'audio_encoder.blocks_a.7.norm2_a.bias', 'audio_encoder.blocks_a.7.norm2_a.weight', 'audio_encoder.blocks_a.7.norm2_v.bias', 'audio_encoder.blocks_a.7.norm2_v.weight', 'audio_encoder.blocks_a.8.attn.proj.bias', 'audio_encoder.blocks_a.8.attn.proj.weight', 'audio_encoder.blocks_a.8.attn.qkv.bias', 'audio_encoder.blocks_a.8.attn.qkv.weight', 'audio_encoder.blocks_a.8.mlp.fc1.bias', 'audio_encoder.blocks_a.8.mlp.fc1.weight', 'audio_encoder.blocks_a.8.mlp.fc2.bias', 'audio_encoder.blocks_a.8.mlp.fc2.weight', 'audio_encoder.blocks_a.8.norm1.bias', 'audio_encoder.blocks_a.8.norm1.weight', 'audio_encoder.blocks_a.8.norm1_a.bias', 'audio_encoder.blocks_a.8.norm1_a.weight', 'audio_encoder.blocks_a.8.norm1_v.bias', 'audio_encoder.blocks_a.8.norm1_v.weight', 'audio_encoder.blocks_a.8.norm2.bias', 'audio_encoder.blocks_a.8.norm2.weight', 'audio_encoder.blocks_a.8.norm2_a.bias', 'audio_encoder.blocks_a.8.norm2_a.weight', 'audio_encoder.blocks_a.8.norm2_v.bias', 'audio_encoder.blocks_a.8.norm2_v.weight', 'audio_encoder.blocks_a.9.attn.proj.bias', 'audio_encoder.blocks_a.9.attn.proj.weight', 'audio_encoder.blocks_a.9.attn.qkv.bias', 'audio_encoder.blocks_a.9.attn.qkv.weight', 'audio_encoder.blocks_a.9.mlp.fc1.bias', 'audio_encoder.blocks_a.9.mlp.fc1.weight', 'audio_encoder.blocks_a.9.mlp.fc2.bias', 'audio_encoder.blocks_a.9.mlp.fc2.weight', 'audio_encoder.blocks_a.9.norm1.bias', 'audio_encoder.blocks_a.9.norm1.weight', 'audio_encoder.blocks_a.9.norm1_a.bias', 'audio_encoder.blocks_a.9.norm1_a.weight', 'audio_encoder.blocks_a.9.norm1_v.bias', 'audio_encoder.blocks_a.9.norm1_v.weight', 'audio_encoder.blocks_a.9.norm2.bias', 'audio_encoder.blocks_a.9.norm2.weight', 'audio_encoder.blocks_a.9.norm2_a.bias', 'audio_encoder.blocks_a.9.norm2_a.weight', 'audio_encoder.blocks_a.9.norm2_v.bias', 'audio_encoder.blocks_a.9.norm2_v.weight', 'audio_encoder.blocks_u.0.attn.proj.bias', 'audio_encoder.blocks_u.0.attn.proj.weight', 'audio_encoder.blocks_u.0.attn.qkv.bias', 'audio_encoder.blocks_u.0.attn.qkv.weight', 'audio_encoder.blocks_u.0.mlp.fc1.bias', 'audio_encoder.blocks_u.0.mlp.fc1.weight', 'audio_encoder.blocks_u.0.mlp.fc2.bias', 'audio_encoder.blocks_u.0.mlp.fc2.weight', 'audio_encoder.blocks_u.0.norm1.bias', 'audio_encoder.blocks_u.0.norm1.weight', 'audio_encoder.blocks_u.0.norm1_a.bias', 'audio_encoder.blocks_u.0.norm1_a.weight', 'audio_encoder.blocks_u.0.norm1_v.bias', 'audio_encoder.blocks_u.0.norm1_v.weight', 'audio_encoder.blocks_u.0.norm2.bias', 'audio_encoder.blocks_u.0.norm2.weight', 'audio_encoder.blocks_u.0.norm2_a.bias', 'audio_encoder.blocks_u.0.norm2_a.weight', 'audio_encoder.blocks_u.0.norm2_v.bias', 'audio_encoder.blocks_u.0.norm2_v.weight', 'audio_encoder.modality_a', 'audio_encoder.norm_a.bias', 'audio_encoder.norm_a.weight', 'audio_encoder.patch_embed_a.proj.bias', 'audio_encoder.patch_embed_a.proj.weight', 'audio_encoder.pos_embed_a', 'audio_proj.1.bias', 'audio_proj.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and config\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "config = T5Config.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# Initialize your custom model\n",
    "customT5 = CustomT5ForConditionalGeneration(config)\n",
    "model = customT5.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a941a9-fc97-48bc-9a1a-e51bdf931278",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.audio_encoder.initialize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cebe8f0d-b587-4471-b67f-3bfd5405b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.audio_proj[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949d95ef-f972-48b1-be03-7d67a4164f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def get_methods_and_params(cls):\n",
    "    methods_and_params = []\n",
    "    for name, member in inspect.getmembers(cls):\n",
    "        if inspect.ismethod(member) or inspect.isfunction(member):\n",
    "            parameters = inspect.signature(member).parameters\n",
    "            param_names = [param for param in parameters.keys() if param != 'self']\n",
    "            methods_and_params.append((name, tuple(param_names)))\n",
    "    return methods_and_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60da6ba5-44dc-437c-9282-a944b0536f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayyss/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('__call__', ('args', 'kwargs')),\n",
       " ('__delattr__', ('name',)),\n",
       " ('__dir__', ()),\n",
       " ('__getattr__', ('name',)),\n",
       " ('__getstate__', ()),\n",
       " ('__init__', ('config',)),\n",
       " ('__repr__', ()),\n",
       " ('__setattr__', ('name', 'value')),\n",
       " ('__setstate__', ('state',)),\n",
       " ('_apply', ('fn', 'recurse')),\n",
       " ('_assisted_decoding',\n",
       "  ('input_ids',\n",
       "   'candidate_generator',\n",
       "   'logits_processor',\n",
       "   'logits_warper',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'streamer',\n",
       "   'model_kwargs')),\n",
       " ('_autoset_attn_implementation',\n",
       "  ('config',\n",
       "   'use_flash_attention_2',\n",
       "   'torch_dtype',\n",
       "   'device_map',\n",
       "   'check_device_map')),\n",
       " ('_backward_compatibility_gradient_checkpointing', ()),\n",
       " ('_beam_sample',\n",
       "  ('input_ids',\n",
       "   'beam_scorer',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'logits_warper',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'model_kwargs')),\n",
       " ('_beam_search',\n",
       "  ('input_ids',\n",
       "   'beam_scorer',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'logits_warper',\n",
       "   'model_kwargs')),\n",
       " ('_call_impl', ('args', 'kwargs')),\n",
       " ('_check_and_enable_flash_attn_2',\n",
       "  ('config',\n",
       "   'torch_dtype',\n",
       "   'device_map',\n",
       "   'check_device_map',\n",
       "   'hard_check_only')),\n",
       " ('_check_and_enable_sdpa', ('config', 'hard_check_only')),\n",
       " ('_constrained_beam_search',\n",
       "  ('input_ids',\n",
       "   'constrained_beam_scorer',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'model_kwargs')),\n",
       " ('_contrastive_search',\n",
       "  ('input_ids',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'streamer',\n",
       "   'model_kwargs')),\n",
       " ('_convert_head_mask_to_5d', ('head_mask', 'num_hidden_layers')),\n",
       " ('_copy_lm_head_original_to_resized',\n",
       "  ('new_lm_head',\n",
       "   'old_lm_head',\n",
       "   'num_tokens_to_copy',\n",
       "   'transposed',\n",
       "   'has_new_lm_head_bias')),\n",
       " ('_create_repo', ('repo_id', 'private', 'token', 'repo_url', 'organization')),\n",
       " ('_dispatch_accelerate_model',\n",
       "  ('device_map', 'max_memory', 'offload_folder', 'offload_index')),\n",
       " ('_expand_inputs_for_generation',\n",
       "  ('expand_size', 'is_encoder_decoder', 'input_ids', 'model_kwargs')),\n",
       " ('_extract_past_from_model_output', ('outputs', 'standardize_cache_format')),\n",
       " ('_from_config', ('config', 'kwargs')),\n",
       " ('_get_backward_hooks', ()),\n",
       " ('_get_backward_pre_hooks', ()),\n",
       " ('_get_candidate_generator',\n",
       "  ('generation_config',\n",
       "   'input_ids',\n",
       "   'inputs_tensor',\n",
       "   'assistant_model',\n",
       "   'logits_processor',\n",
       "   'model_kwargs')),\n",
       " ('_get_decoder_start_token_id', ('decoder_start_token_id', 'bos_token_id')),\n",
       " ('_get_files_timestamps', ('working_dir',)),\n",
       " ('_get_initial_cache_position', ('input_ids', 'model_kwargs')),\n",
       " ('_get_logits_processor',\n",
       "  ('generation_config',\n",
       "   'input_ids_seq_length',\n",
       "   'encoder_input_ids',\n",
       "   'prefix_allowed_tokens_fn',\n",
       "   'logits_processor',\n",
       "   'device',\n",
       "   'model_kwargs',\n",
       "   'negative_prompt_ids',\n",
       "   'negative_prompt_attention_mask')),\n",
       " ('_get_logits_warper', ('generation_config',)),\n",
       " ('_get_name', ()),\n",
       " ('_get_no_split_modules', ('device_map',)),\n",
       " ('_get_resized_embeddings',\n",
       "  ('old_embeddings', 'new_num_tokens', 'pad_to_multiple_of')),\n",
       " ('_get_resized_lm_head', ('old_lm_head', 'new_num_tokens', 'transposed')),\n",
       " ('_get_static_cache', ('max_batch_size', 'max_cache_len')),\n",
       " ('_get_stopping_criteria',\n",
       "  ('generation_config', 'stopping_criteria', 'tokenizer', 'kwargs')),\n",
       " ('_greedy_search',\n",
       "  ('input_ids',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'streamer',\n",
       "   'model_kwargs')),\n",
       " ('_group_beam_search',\n",
       "  ('input_ids',\n",
       "   'beam_scorer',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'model_kwargs')),\n",
       " ('_has_unfinished_sequences',\n",
       "  ('this_peer_finished', 'synced_gpus', 'device')),\n",
       " ('_hook_rss_memory_post_forward', ('module', 'args', 'kwargs')),\n",
       " ('_hook_rss_memory_pre_forward', ('module', 'args', 'kwargs')),\n",
       " ('_init_weights', ('module',)),\n",
       " ('_initialize_weights', ('module',)),\n",
       " ('_load_from_state_dict',\n",
       "  ('state_dict',\n",
       "   'prefix',\n",
       "   'local_metadata',\n",
       "   'strict',\n",
       "   'missing_keys',\n",
       "   'unexpected_keys',\n",
       "   'error_msgs')),\n",
       " ('_load_pretrained_model',\n",
       "  ('model',\n",
       "   'state_dict',\n",
       "   'loaded_keys',\n",
       "   'resolved_archive_file',\n",
       "   'pretrained_model_name_or_path',\n",
       "   'ignore_mismatched_sizes',\n",
       "   'sharded_metadata',\n",
       "   '_fast_init',\n",
       "   'low_cpu_mem_usage',\n",
       "   'device_map',\n",
       "   'offload_folder',\n",
       "   'offload_state_dict',\n",
       "   'dtype',\n",
       "   'hf_quantizer',\n",
       "   'keep_in_fp32_modules',\n",
       "   'gguf_path')),\n",
       " ('_load_pretrained_model_low_mem',\n",
       "  ('model',\n",
       "   'loaded_state_dict_keys',\n",
       "   'resolved_archive_file',\n",
       "   'start_prefix',\n",
       "   'hf_quantizer')),\n",
       " ('_maybe_initialize_input_ids_for_generation',\n",
       "  ('inputs', 'bos_token_id', 'model_kwargs')),\n",
       " ('_maybe_warn_non_full_backward_hook', ('inputs', 'result', 'grad_fn')),\n",
       " ('_merge_criteria_processor_list', ('default_list', 'custom_list')),\n",
       " ('_named_members',\n",
       "  ('get_members_fn', 'prefix', 'recurse', 'remove_duplicate')),\n",
       " ('_prepare_attention_mask_for_generation',\n",
       "  ('inputs', 'pad_token_id', 'eos_token_id')),\n",
       " ('_prepare_decoder_input_ids_for_generation',\n",
       "  ('batch_size',\n",
       "   'model_input_name',\n",
       "   'model_kwargs',\n",
       "   'decoder_start_token_id',\n",
       "   'device')),\n",
       " ('_prepare_encoder_decoder_kwargs_for_generation',\n",
       "  ('inputs_tensor', 'model_kwargs', 'model_input_name', 'generation_config')),\n",
       " ('_prepare_generated_length',\n",
       "  ('generation_config',\n",
       "   'has_default_max_length',\n",
       "   'has_default_min_length',\n",
       "   'model_input_name',\n",
       "   'input_ids_length',\n",
       "   'inputs_tensor')),\n",
       " ('_prepare_generation_config', ('generation_config', 'kwargs')),\n",
       " ('_prepare_model_inputs', ('inputs', 'bos_token_id', 'model_kwargs')),\n",
       " ('_prepare_special_tokens',\n",
       "  ('generation_config', 'kwargs_has_attention_mask', 'device')),\n",
       " ('_register_load_state_dict_pre_hook', ('hook', 'with_module')),\n",
       " ('_register_state_dict_hook', ('hook',)),\n",
       " ('_reorder_cache', ('past_key_values', 'beam_idx')),\n",
       " ('_replicate_for_data_parallel', ()),\n",
       " ('_resize_token_embeddings', ('new_num_tokens', 'pad_to_multiple_of')),\n",
       " ('_sample',\n",
       "  ('input_ids',\n",
       "   'audio_input',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'streamer',\n",
       "   'logits_warper',\n",
       "   'model_kwargs')),\n",
       " ('_save_to_state_dict', ('destination', 'prefix', 'keep_vars')),\n",
       " ('_set_default_torch_dtype', ('dtype',)),\n",
       " ('_set_gradient_checkpointing', ('enable', 'gradient_checkpointing_func')),\n",
       " ('_shift_right', ('input_ids',)),\n",
       " ('_slow_forward', ('input', 'kwargs')),\n",
       " ('_temporary_reorder_cache', ('past_key_values', 'beam_idx')),\n",
       " ('_tie_encoder_decoder_weights',\n",
       "  ('encoder', 'decoder', 'base_model_prefix', 'base_encoder_name')),\n",
       " ('_tie_or_clone_weights', ('output_embeddings', 'input_embeddings')),\n",
       " ('_tie_weights', ()),\n",
       " ('_update_model_kwargs_for_generation',\n",
       "  ('outputs',\n",
       "   'model_kwargs',\n",
       "   'is_encoder_decoder',\n",
       "   'standardize_cache_format',\n",
       "   'num_new_tokens')),\n",
       " ('_upload_modified_files',\n",
       "  ('working_dir',\n",
       "   'repo_id',\n",
       "   'files_timestamps',\n",
       "   'commit_message',\n",
       "   'token',\n",
       "   'create_pr',\n",
       "   'revision',\n",
       "   'commit_description')),\n",
       " ('_validate_generated_length',\n",
       "  ('generation_config', 'input_ids_length', 'has_default_max_length')),\n",
       " ('_validate_model_class', ()),\n",
       " ('_validate_model_kwargs', ('model_kwargs',)),\n",
       " ('_wrapped_call_impl', ('args', 'kwargs')),\n",
       " ('active_adapter', ()),\n",
       " ('active_adapters', ()),\n",
       " ('add_adapter', ('adapter_config', 'adapter_name')),\n",
       " ('add_memory_hooks', ()),\n",
       " ('add_model_tags', ('tags',)),\n",
       " ('add_module', ('name', 'module')),\n",
       " ('apply', ('fn',)),\n",
       " ('bfloat16', ()),\n",
       " ('buffers', ('recurse',)),\n",
       " ('can_generate', ()),\n",
       " ('children', ()),\n",
       " ('compile', ('args', 'kwargs')),\n",
       " ('compute_transition_scores',\n",
       "  ('sequences', 'scores', 'beam_indices', 'normalize_logits')),\n",
       " ('cpu', ()),\n",
       " ('create_extended_attention_mask_for_decoder',\n",
       "  ('input_shape', 'attention_mask', 'device')),\n",
       " ('cuda', ('device',)),\n",
       " ('deparallelize', ()),\n",
       " ('dequantize', ()),\n",
       " ('disable_adapters', ()),\n",
       " ('disable_input_require_grads', ()),\n",
       " ('double', ()),\n",
       " ('enable_adapters', ()),\n",
       " ('enable_input_require_grads', ()),\n",
       " ('estimate_tokens', ('input_dict',)),\n",
       " ('eval', ()),\n",
       " ('extra_repr', ()),\n",
       " ('float', ('args',)),\n",
       " ('floating_point_ops', ('input_dict', 'exclude_embeddings')),\n",
       " ('forward',\n",
       "  ('audio_input',\n",
       "   'input_ids',\n",
       "   'attention_mask',\n",
       "   'decoder_input_ids',\n",
       "   'decoder_attention_mask',\n",
       "   'head_mask',\n",
       "   'decoder_head_mask',\n",
       "   'cross_attn_head_mask',\n",
       "   'encoder_outputs',\n",
       "   'past_key_values',\n",
       "   'inputs_embeds',\n",
       "   'decoder_inputs_embeds',\n",
       "   'labels',\n",
       "   'use_cache',\n",
       "   'output_attentions',\n",
       "   'output_hidden_states',\n",
       "   'return_dict')),\n",
       " ('from_pretrained',\n",
       "  ('pretrained_model_name_or_path',\n",
       "   'model_args',\n",
       "   'config',\n",
       "   'cache_dir',\n",
       "   'ignore_mismatched_sizes',\n",
       "   'force_download',\n",
       "   'local_files_only',\n",
       "   'token',\n",
       "   'revision',\n",
       "   'use_safetensors',\n",
       "   'kwargs')),\n",
       " ('generate',\n",
       "  ('inputs',\n",
       "   'audio_input',\n",
       "   'generation_config',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'prefix_allowed_tokens_fn',\n",
       "   'synced_gpus',\n",
       "   'assistant_model',\n",
       "   'streamer',\n",
       "   'negative_prompt_ids',\n",
       "   'negative_prompt_attention_mask',\n",
       "   'kwargs')),\n",
       " ('get_adapter_state_dict', ('adapter_name',)),\n",
       " ('get_buffer', ('target',)),\n",
       " ('get_decoder', ()),\n",
       " ('get_encoder', ()),\n",
       " ('get_extended_attention_mask',\n",
       "  ('attention_mask', 'input_shape', 'device', 'dtype')),\n",
       " ('get_extra_state', ()),\n",
       " ('get_head_mask', ('head_mask', 'num_hidden_layers', 'is_attention_chunked')),\n",
       " ('get_input_embeddings', ()),\n",
       " ('get_memory_footprint', ('return_buffers',)),\n",
       " ('get_output_embeddings', ()),\n",
       " ('get_parameter', ('target',)),\n",
       " ('get_position_embeddings', ()),\n",
       " ('get_submodule', ('target',)),\n",
       " ('gradient_checkpointing_disable', ()),\n",
       " ('gradient_checkpointing_enable', ('gradient_checkpointing_kwargs',)),\n",
       " ('half', ('args',)),\n",
       " ('init_weights', ()),\n",
       " ('invert_attention_mask', ('encoder_attention_mask',)),\n",
       " ('ipu', ('device',)),\n",
       " ('load_adapter',\n",
       "  ('peft_model_id',\n",
       "   'adapter_name',\n",
       "   'revision',\n",
       "   'token',\n",
       "   'device_map',\n",
       "   'max_memory',\n",
       "   'offload_folder',\n",
       "   'offload_index',\n",
       "   'peft_config',\n",
       "   'adapter_state_dict',\n",
       "   'adapter_kwargs')),\n",
       " ('load_state_dict', ('state_dict', 'strict', 'assign')),\n",
       " ('load_tf_weights', ('config', 'tf_checkpoint_path')),\n",
       " ('modules', ()),\n",
       " ('named_buffers', ('prefix', 'recurse', 'remove_duplicate')),\n",
       " ('named_children', ()),\n",
       " ('named_modules', ('memo', 'prefix', 'remove_duplicate')),\n",
       " ('named_parameters', ('prefix', 'recurse', 'remove_duplicate')),\n",
       " ('num_parameters', ('only_trainable', 'exclude_embeddings')),\n",
       " ('parallelize', ('device_map',)),\n",
       " ('parameters', ('recurse',)),\n",
       " ('post_init', ()),\n",
       " ('prepare_decoder_input_ids_from_labels', ('labels',)),\n",
       " ('prepare_inputs_for_generation',\n",
       "  ('input_ids',\n",
       "   'audio_input',\n",
       "   'past_key_values',\n",
       "   'attention_mask',\n",
       "   'head_mask',\n",
       "   'decoder_head_mask',\n",
       "   'decoder_attention_mask',\n",
       "   'cross_attn_head_mask',\n",
       "   'use_cache',\n",
       "   'encoder_outputs',\n",
       "   'kwargs')),\n",
       " ('prune_heads', ('heads_to_prune',)),\n",
       " ('push_to_hub',\n",
       "  ('repo_id',\n",
       "   'use_temp_dir',\n",
       "   'commit_message',\n",
       "   'private',\n",
       "   'token',\n",
       "   'max_shard_size',\n",
       "   'create_pr',\n",
       "   'safe_serialization',\n",
       "   'revision',\n",
       "   'commit_description',\n",
       "   'tags',\n",
       "   'deprecated_kwargs')),\n",
       " ('register_backward_hook', ('hook',)),\n",
       " ('register_buffer', ('name', 'tensor', 'persistent')),\n",
       " ('register_for_auto_class', ('auto_class',)),\n",
       " ('register_forward_hook', ('hook', 'prepend', 'with_kwargs', 'always_call')),\n",
       " ('register_forward_pre_hook', ('hook', 'prepend', 'with_kwargs')),\n",
       " ('register_full_backward_hook', ('hook', 'prepend')),\n",
       " ('register_full_backward_pre_hook', ('hook', 'prepend')),\n",
       " ('register_load_state_dict_post_hook', ('hook',)),\n",
       " ('register_module', ('name', 'module')),\n",
       " ('register_parameter', ('name', 'param')),\n",
       " ('register_state_dict_pre_hook', ('hook',)),\n",
       " ('requires_grad_', ('requires_grad',)),\n",
       " ('reset_memory_hooks_state', ()),\n",
       " ('resize_position_embeddings', ('new_num_position_embeddings',)),\n",
       " ('resize_token_embeddings', ('new_num_tokens', 'pad_to_multiple_of')),\n",
       " ('retrieve_modules_from_names', ('names', 'add_prefix', 'remove_prefix')),\n",
       " ('reverse_bettertransformer', ()),\n",
       " ('save_pretrained',\n",
       "  ('save_directory',\n",
       "   'is_main_process',\n",
       "   'state_dict',\n",
       "   'save_function',\n",
       "   'push_to_hub',\n",
       "   'max_shard_size',\n",
       "   'safe_serialization',\n",
       "   'variant',\n",
       "   'token',\n",
       "   'save_peft_format',\n",
       "   'kwargs')),\n",
       " ('set_adapter', ('adapter_name',)),\n",
       " ('set_extra_state', ('state',)),\n",
       " ('set_input_embeddings', ('new_embeddings',)),\n",
       " ('set_output_embeddings', ('new_embeddings',)),\n",
       " ('share_memory', ()),\n",
       " ('state_dict', ('args', 'destination', 'prefix', 'keep_vars')),\n",
       " ('tie_weights', ()),\n",
       " ('to', ('args', 'kwargs')),\n",
       " ('to_bettertransformer', ()),\n",
       " ('to_empty', ('device', 'recurse')),\n",
       " ('train', ('mode',)),\n",
       " ('type', ('dst_type',)),\n",
       " ('warn_if_padding_and_no_attention_mask', ('input_ids', 'attention_mask')),\n",
       " ('xpu', ('device',)),\n",
       " ('zero_grad', ('set_to_none',))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_methods_and_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c06a6c2-48ce-4c66-b0c1-8eddebb2d8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869658624"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf2146e1-efdc-48a7-a084-09b3d379b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./data/toy_dataset/openaqa_toy.json\"\n",
    "\n",
    "with open(file, \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c435d-eafe-4f64-9f28-6dd0e5c8f16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "116d9065-5f15-4d5a-8178-9dd6d3f4b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/toy_dataset/audio/4tnW9atZKo0.flac\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['audio_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff4e5abb-b769-4452-aeb9-804142cc8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    path = data[i]['audio_id']\n",
    "    exten = path[len(path)-4:]\n",
    "    if exten == \"flac\" or exten == \".wav\":\n",
    "        mini_path = \"\"\n",
    "        for j in range(len(path)-1, -1, -1):\n",
    "            if path[j] == \"/\":\n",
    "                break\n",
    "            mini_path += path[j]\n",
    "        data[i]['audio_id'] = \"./data/toy_dataset/audio/\" + mini_path[::-1]\n",
    "        \n",
    "with open(file, \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67caa384-5ded-4ca9-a2bf-23e36042eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=file, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "035c3b52-b557-4d9a-88b5-720125da5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80ebd258-ca28-4e60-8659-e1d12fd45e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5044, 6), (1262, 6))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape, test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "039bef0a-76ac-412a-9ee3-31bc437727d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What mood or atmosphere does the audio clip convey?',\n",
       " 'input': '',\n",
       " 'audio_id': './data/toy_dataset/audio/82070.flac',\n",
       " 'dataset': 'freesound_10s',\n",
       " 'task': 'open-ended question',\n",
       " 'output': 'The audio clip conveys a humorous mood or atmosphere.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c4c270-b7fc-4f13-a362-84f0e23ee5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filterbank\n",
    "def load_audio(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    audio_info = 'Original input audio length {:.2f} seconds, number of channels: {:d}, sampling rate: {:d}.'.format(waveform.shape[1]/sample_rate, waveform.shape[0], sample_rate)\n",
    "    if waveform.shape[0] != 1:\n",
    "        waveform = waveform[0].unsqueeze(0)\n",
    "        audio_info += ' Only the first channel is used.'\n",
    "    if sample_rate == 16000:\n",
    "        pass\n",
    "    else:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=sample_rate, new_freq=16000)\n",
    "        sample_rate = 16000\n",
    "        audio_info += ' Resample to 16000Hz.'\n",
    "    waveform = waveform - waveform.mean()\n",
    "    fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sample_rate,\n",
    "                                              use_energy=False, window_type='hanning',\n",
    "                                              num_mel_bins=128, dither=0.0, frame_shift=10)\n",
    "    target_length = 1024\n",
    "    n_frames = fbank.shape[0]\n",
    "    p = target_length - n_frames\n",
    "    if p > 0:\n",
    "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "        fbank = m(fbank)\n",
    "    elif p < 0:\n",
    "        fbank = fbank[0:target_length, :]\n",
    "    # normalize the fbank\n",
    "    fbank = (fbank + 5.081) / 4.4849\n",
    "    return fbank, audio_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "726a253e-ba72-41a3-94d3-e46f95459c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def return_audio(path):\n",
    "\n",
    "    cur_audio_input, audio_info = load_audio(path)\n",
    "    cur_audio_input = cur_audio_input.unsqueeze(0)\n",
    "    \n",
    "    # projecting to 1024 input embedding dimension for T5\n",
    "    audio_proj = nn.Sequential(nn.LayerNorm(768, elementwise_affine=False), nn.Linear(768, 1024))\n",
    "    audio_input = audio_encoder(cur_audio_input)  # [B, 512, 768]\n",
    "    audio_input = audio_input.reshape(audio_input.shape[0], 8, 64, audio_input.shape[-1])\n",
    "    audio_input = torch.mean(audio_input, dim=1)  # mean pool over the frequency dimension # [B, 64, 768]\n",
    "    audio_input = torch.nn.functional.avg_pool2d(audio_input, (2, 1)) #[B, 32, 768]\n",
    "    # hard norm to 50\n",
    "    audio_input = audio_input / 50\n",
    "    audio_input = audio_proj(audio_input) #[B, 32, 1024]\n",
    "    \n",
    "    return audio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c664636-70a2-44d0-9451-b6a8c321ecf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abfcde6e-8a70-4175-8b7e-3fe3c7149121",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42670ff5-a524-4846-9f6b-c93b944ed181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CombinedEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, model, device, max_length=512):\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instruction = self.data[idx]['instruction']\n",
    "        audio_path = self.data[idx]['audio_id']\n",
    "        label = self.data[idx]['output']\n",
    "        \n",
    "        input_ids = self.tokenizer(instruction, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        decoder_input_ids = self.tokenizer(label, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        audio_bank, audio_info = load_audio(audio_path)\n",
    "        audio_bank = audio_bank.to(self.device)\n",
    "        \n",
    "        return input_ids, decoder_input_ids.squeeze(0), audio_bank\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    input_ids = [item[0].squeeze(0) for item in batch]  # Remove unnecessary dim\n",
    "    decoder_input_ids = [item[1] for item in batch]\n",
    "    audio_input = [item[2] for item in batch]\n",
    "\n",
    "    # Pad input_ids and decoder_input_ids to the maximum length in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    decoder_input_ids_padded = pad_sequence(decoder_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Stack audio inputs directly if they have the same size\n",
    "    audio_input_stacked = torch.stack(audio_input)\n",
    "    return input_ids_padded, audio_input_stacked, decoder_input_ids_padded\n",
    "\n",
    "\n",
    "dataset = CombinedEmbeddingsDataset(train_dataset, tokenizer, model, device)\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e7478-a346-430f-b431-1ad6ece42a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.7653350830078125\n",
      "loss: 3.4598305225372314\n",
      "loss: 3.3965837955474854\n",
      "loss: 3.5168426036834717\n",
      "loss: 3.1912124156951904\n",
      "loss: 3.251225709915161\n",
      "loss: 2.9739413261413574\n",
      "loss: 2.9704782962799072\n",
      "loss: 2.6957931518554688\n",
      "loss: 2.7317123413085938\n",
      "loss: 2.475053310394287\n",
      "loss: 2.4693453311920166\n"
     ]
    }
   ],
   "source": [
    "# Example labels (target sequence for training)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Iterate through the DataLoader\n",
    "    for i, (input_ids, audio_bank, decoder_input_ids) in enumerate(itertools.islice(dataloader, 2)):\n",
    "        if torch.isnan(input_ids).any() or torch.isnan(audio_bank).any() or torch.isnan(decoder_input_ids).any():\n",
    "            print(\"NaN detected in input tensors\")\n",
    "        #print(\"from loop:\",input_ids.shape)\n",
    "        #print(\"from loop audio:\", audio_bank.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, audio_input=audio_bank,labels=decoder_input_ids)\n",
    "       #print(outputs)\n",
    "        loss = outputs.loss\n",
    "        print(\"loss:\", loss.item())\n",
    "        #logits = outputs.logits  \n",
    "        #predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        #decoded_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        #print(decoded_text)\n",
    "        #predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70b4ebe8-5573-455b-8fe1-c888b007921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, \"working_model_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad61e18f-41af-4dcd-8fed-676f33666e6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworking_model_2.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[1;32m   1026\u001b[0m                      map_location,\n\u001b[1;32m   1027\u001b[0m                      pickle_module,\n\u001b[1;32m   1028\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[1;32m   1029\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcuda(device)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/_utils.py:114\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "checkpoint = torch.load(\"working_model_2.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10efe568-3e31-4845-af27-f422fd7e0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cave_model import CAVMAEFTAudio\n",
    "audio_encoder = CAVMAEFTAudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c1a5635-50ab-4e71-8fb5-4b2c1355ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from CAVE inside: tensor([[[-2.2843, -2.4218, -2.0455,  ..., -0.9920, -1.5722, -2.3284],\n",
      "         [-1.8898, -2.3895, -1.6217,  ..., -0.2275, -1.0407, -2.1476],\n",
      "         [-1.6393, -2.1380, -1.3702,  ...,  0.0030, -0.7107, -1.5814],\n",
      "         ...,\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329]]])\n",
      "shape before CAVE: torch.Size([1, 1024, 128])\n",
      "from CAVE before patch: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]])\n",
      "patch embed: torch.Size([1, 1, 128, 1024])\n",
      "before convo: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]])\n",
      "from CAVE after patch: tensor([[[ 2.6611, -1.2217, -0.2890,  ..., -0.4917, -0.9088,  0.5112],\n",
      "         [ 2.8197, -1.7707, -0.3036,  ..., -0.0717, -1.1140,  0.7702],\n",
      "         [ 3.2779, -2.0465, -0.2818,  ..., -0.2168, -1.5610,  1.0363],\n",
      "         ...,\n",
      "         [-1.7904,  1.0632,  0.1043,  ...,  0.2845,  0.6632, -0.7552],\n",
      "         [-1.7904,  1.0632,  0.1043,  ...,  0.2845,  0.6632, -0.7552],\n",
      "         [-1.7904,  1.0632,  0.1043,  ...,  0.2845,  0.6632, -0.7552]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "from CAVE inside 2: tensor([[[ 2.6773, -1.2612, -0.2841,  ...,  0.4644,  0.1219,  1.4585],\n",
      "         [ 3.6774, -0.9949,  0.4899,  ...,  0.8844, -0.0833,  1.7175],\n",
      "         [ 4.2034, -1.1418,  0.6930,  ...,  0.7394, -0.5303,  1.9836],\n",
      "         ...,\n",
      "         [-2.7403,  2.0234, -0.7949,  ...,  1.2407,  1.6939,  0.1920],\n",
      "         [-2.5134,  1.5836, -0.1096,  ...,  1.2407,  1.6939,  0.1920],\n",
      "         [-1.6068,  0.6724,  0.7441,  ...,  1.2407,  1.6939,  0.1920]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "shape before putting in block: torch.Size([1, 512, 768])\n",
      "from CAVE inside 3: tensor([[[-1.9227, -0.8392, -0.0687,  ..., -0.0199, -0.1403, -0.5158],\n",
      "         [-1.8927, -0.8580, -0.0356,  ...,  0.0313, -0.1316, -0.5077],\n",
      "         [-1.8534, -0.8358,  0.0105,  ...,  0.0134, -0.0942, -0.5143],\n",
      "         ...,\n",
      "         [-2.3141, -0.5856, -0.3207,  ..., -0.3211,  0.0934, -0.4715],\n",
      "         [-2.3242, -0.6239, -0.3015,  ..., -0.2697,  0.0892, -0.4805],\n",
      "         [-2.3116, -0.6845, -0.2764,  ..., -0.2048,  0.0723, -0.4951]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "from CAVE inside 4: tensor([[[-1.7023, -0.7344, -0.0461,  ..., -0.0025, -0.1100, -0.4455],\n",
      "         [-1.6768, -0.7517, -0.0163,  ...,  0.0435, -0.1022, -0.4384],\n",
      "         [-1.6403, -0.7311,  0.0251,  ...,  0.0277, -0.0684, -0.4439],\n",
      "         ...,\n",
      "         [-2.0549, -0.5034, -0.2656,  ..., -0.2660,  0.1061, -0.4010],\n",
      "         [-2.0651, -0.5383, -0.2489,  ..., -0.2203,  0.1020, -0.4096],\n",
      "         [-2.0548, -0.5933, -0.2267,  ..., -0.1624,  0.0865, -0.4231]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7023, -0.7344, -0.0461,  ..., -0.0025, -0.1100, -0.4455],\n",
       "         [-1.6768, -0.7517, -0.0163,  ...,  0.0435, -0.1022, -0.4384],\n",
       "         [-1.6403, -0.7311,  0.0251,  ...,  0.0277, -0.0684, -0.4439],\n",
       "         ...,\n",
       "         [-2.0549, -0.5034, -0.2656,  ..., -0.2660,  0.1061, -0.4010],\n",
       "         [-2.0651, -0.5383, -0.2489,  ..., -0.2203,  0.1020, -0.4096],\n",
       "         [-2.0548, -0.5933, -0.2267,  ..., -0.1624,  0.0865, -0.4231]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"./data/toy_dataset/audio/577026.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0)\n",
    "audio_input = audio_encoder(cur_audio_input) \n",
    "audio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4241216e-6a36-4ead-b4c7-1b3f7fc434f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from CAVE inside: tensor([[[-2.2843, -2.4218, -2.0455,  ..., -0.9920, -1.5722, -2.3284],\n",
      "         [-1.8898, -2.3895, -1.6217,  ..., -0.2275, -1.0407, -2.1476],\n",
      "         [-1.6393, -2.1380, -1.3702,  ...,  0.0030, -0.7107, -1.5814],\n",
      "         ...,\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329]]],\n",
      "       device='cuda:0')\n",
      "shape before CAVE: torch.Size([1, 1024, 128])\n",
      "from CAVE before patch: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]],\n",
      "       device='cuda:0')\n",
      "patch embed: torch.Size([1, 1, 128, 1024])\n",
      "before convo: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]],\n",
      "       device='cuda:0')\n",
      "from CAVE after patch: tensor([[[-1.9091, -0.8451,  0.2472,  ...,  0.1447, -0.0661,  0.8762],\n",
      "         [-2.0660, -1.2898,  0.4945,  ...,  0.3127, -1.1853,  1.2340],\n",
      "         [-2.3570, -1.1388,  0.7489,  ...,  0.3829, -1.1931,  0.9730],\n",
      "         ...,\n",
      "         [ 0.7995,  0.6390, -0.5792,  ..., -0.1193,  0.6473, -0.6660],\n",
      "         [ 0.7995,  0.6390, -0.5792,  ..., -0.1193,  0.6473, -0.6660],\n",
      "         [ 0.7995,  0.6390, -0.5792,  ..., -0.1193,  0.6473, -0.6660]]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "from CAVE inside 2: tensor([[[-1.9106, -0.8678,  0.2206,  ...,  1.1831,  0.9352,  1.8581],\n",
      "         [-1.2259, -0.4973,  1.2565,  ...,  1.3511, -0.1839,  2.2159],\n",
      "         [-1.4492, -0.2173,  1.6922,  ...,  1.4213, -0.1917,  1.9549],\n",
      "         ...,\n",
      "         [-0.1680,  1.6160, -1.5098,  ...,  0.9191,  1.6487,  0.3159],\n",
      "         [ 0.0589,  1.1762, -0.8246,  ...,  0.9191,  1.6487,  0.3159],\n",
      "         [ 0.9654,  0.2650,  0.0291,  ...,  0.9191,  1.6487,  0.3159]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "shape before putting in block: torch.Size([1, 512, 768])\n",
      "from CAVE inside 3: tensor([[[ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262],\n",
      "         [ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262],\n",
      "         [ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262],\n",
      "         ...,\n",
      "         [ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262],\n",
      "         [ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262],\n",
      "         [ 1.0819,  0.4062, -0.0914,  ...,  1.1714, -1.0042, -0.5262]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "from CAVE inside 4: tensor([[[ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739],\n",
      "         [ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739],\n",
      "         [ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739],\n",
      "         ...,\n",
      "         [ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739],\n",
      "         [ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739],\n",
      "         [ 0.9977,  0.3794, -0.0761,  ...,  1.0796, -0.9114, -0.4739]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "Audio: tensor([[[5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41],\n",
      "         [5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41],\n",
      "         [5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41],\n",
      "         ...,\n",
      "         [5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41],\n",
      "         [5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41],\n",
      "         [5.2466e+15, 1.3509e-35, 8.6795e-37,  ..., 4.0729e-41,\n",
      "          6.0559e-37, 4.0729e-41]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "input_embeds: tensor([[[ 7.5833, -5.9968, -8.7719,  ..., -1.7548, -1.4999,  0.8449],\n",
      "         [ 8.0296,  3.5735, -0.2749,  ..., -0.3696,  0.0123, -2.4261],\n",
      "         [-7.9453, -3.8064,  5.7910,  ...,  2.7917,  1.0186,  1.6824],\n",
      "         ...,\n",
      "         [-7.1989, -7.0903, 10.7553,  ...,  7.1836,  5.0034, -9.7338],\n",
      "         [ 1.5147,  3.0822, -0.0883,  ...,  0.4944,  3.2871, -6.7017],\n",
      "         [-2.7021, -2.1235,  0.5805,  ..., -2.3538, -4.8025, -5.8202]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "concat shape: torch.Size([1, 43, 1024])\n"
     ]
    }
   ],
   "source": [
    "file = \"./data/toy_dataset/audio/577026.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0).to(device)\n",
    "prompt_text = \"what can be infered from this audio following\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "target_text = \"It can be inferred that the audio is a recording of a musical performance or a rehearsal.The combination of music, speech,\"\\\n",
    "\"and sound effects suggests that the audio is a representation of a musical performance or a rehearsal, where the music is being played\"\\\n",
    "\"and the performers are practicing their performance or rehearsing.\"\n",
    "target_ids = tokenizer(target_text, return_tensors='pt').input_ids.to(device)\n",
    "decoder_input_ids = model._shift_right(target_ids).to(device)\n",
    "\n",
    "outputs = model(input_ids=input_ids, audio_input=cur_audio_input,labels=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "657ae9e4-a045-46be-9513-55aa19c489ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There\n"
     ]
    }
   ],
   "source": [
    "file = \"./data/toy_dataset/audio/3CHor3uzS00.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0).to(device)\n",
    "prompt_text = \"what can be infered from this audio following\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "target_text = \"\"\n",
    "target_ids = tokenizer(target_text, return_tensors='pt').input_ids.to(device)\n",
    "decoder_input_ids = model._shift_right(target_ids).to(device)\n",
    "\n",
    "outputs = model(input_ids=input_ids, audio_input=cur_audio_input,labels=decoder_input_ids)\n",
    "logits = outputs.logits  \n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "decoded_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c34993b-8ea6-4622-8894-f5d937fb29e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayyss/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (11) must match the size of tensor b (512) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculation for inference\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, audio_input\u001b[38;5;241m=\u001b[39mcur_audio_input)\n\u001b[1;32m     10\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/generation/utils.py:1759\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, audio_input, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1751\u001b[0m             input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1752\u001b[0m                 input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1753\u001b[0m                 expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1754\u001b[0m                 is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1755\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1756\u001b[0m             )\n\u001b[1;32m   1758\u001b[0m             \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1759\u001b[0m             result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   1760\u001b[0m                 input_ids,\n\u001b[1;32m   1761\u001b[0m \t\taudio_input,\n\u001b[1;32m   1762\u001b[0m                 logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1763\u001b[0m                 logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[1;32m   1764\u001b[0m                 stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1765\u001b[0m                 generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   1766\u001b[0m                 synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1767\u001b[0m                 streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1768\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1769\u001b[0m             )\n\u001b[1;32m   1771\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1772\u001b[0m             \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m             prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1774\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m             )\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/generation/utils.py:2400\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, audio_input, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2397\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, audio_input, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2399\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2400\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[1;32m   2401\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2402\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2403\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   2404\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2405\u001b[0m )\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2408\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 215\u001b[0m, in \u001b[0;36mCustomT5ForConditionalGeneration.forward\u001b[0;34m(self, audio_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    212\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    216\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m    217\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m    218\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m    219\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    220\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    221\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    222\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m    223\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m    224\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    225\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    226\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    227\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    230\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1107\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1093\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1094\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         output_attentions,\n\u001b[1;32m   1105\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m   1108\u001b[0m         hidden_states,\n\u001b[1;32m   1109\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1110\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m   1111\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1112\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1113\u001b[0m         encoder_decoder_position_bias\u001b[38;5;241m=\u001b[39mencoder_decoder_position_bias,\n\u001b[1;32m   1114\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m   1115\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m   1116\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m   1117\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1118\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:717\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 717\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m1\u001b[39m](\n\u001b[1;32m    718\u001b[0m     hidden_states,\n\u001b[1;32m    719\u001b[0m     key_value_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    720\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m    721\u001b[0m     position_bias\u001b[38;5;241m=\u001b[39mencoder_decoder_position_bias,\n\u001b[1;32m    722\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m    723\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mcross_attn_past_key_value,\n\u001b[1;32m    724\u001b[0m     query_length\u001b[38;5;241m=\u001b[39mquery_length,\n\u001b[1;32m    725\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    726\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    727\u001b[0m )\n\u001b[1;32m    728\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:628\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    617\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    625\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m ):\n\u001b[1;32m    627\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 628\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncDecAttention(\n\u001b[1;32m    629\u001b[0m         normed_hidden_states,\n\u001b[1;32m    630\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    631\u001b[0m         key_value_states\u001b[38;5;241m=\u001b[39mkey_value_states,\n\u001b[1;32m    632\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m    633\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m    634\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    635\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    636\u001b[0m         query_length\u001b[38;5;241m=\u001b[39mquery_length,\n\u001b[1;32m    637\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    638\u001b[0m     )\n\u001b[1;32m    639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    640\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:544\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    541\u001b[0m         position_bias \u001b[38;5;241m=\u001b[39m position_bias[:, :, \u001b[38;5;241m-\u001b[39mhidden_states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) :, :]\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 544\u001b[0m         position_bias \u001b[38;5;241m=\u001b[39m position_bias \u001b[38;5;241m+\u001b[39m mask  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruned_heads:\n\u001b[1;32m    547\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(position_bias\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (11) must match the size of tensor b (512) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "file = \"./data/toy_dataset/audio/3Dbziwli7h4_000043.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0).to(device)\n",
    "prompt_text = \"what can be infered from this following audio\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    outputs = model.generate(input_ids=input_ids, audio_input=cur_audio_input)\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff4d13-16a4-40af-8066-5e834b39614a",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d429a4ed-867c-4a2c-8cb3-22beb5a44985",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "audio input cannot be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m     attention_mask[:, \u001b[38;5;241m-\u001b[39mpadding_length:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs_embeds\u001b[38;5;241m=\u001b[39mcombined_embeddings, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,labels\u001b[38;5;241m=\u001b[39mdecoder_input_ids)\n\u001b[1;32m     42\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits  \n\u001b[1;32m     44\u001b[0m predicted_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[65], line 82\u001b[0m, in \u001b[0;36mCustomT5ForConditionalGeneration.forward\u001b[0;34m(self, audio_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     80\u001b[0m     audio_length \u001b[38;5;241m=\u001b[39m audio_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio input cannot be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Custom: get embeddings \u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: audio input cannot be empty"
     ]
    }
   ],
   "source": [
    "audio_encoder = CAVMAEFTAudio()\n",
    "prompt_text = \"what can be infered from this audio following\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "audio_input = return_audio(\"./data/toy_dataset/audio/_4X8RNeWeDI.flac\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_embeddings = model.shared(input_ids)  # Shape: (1, sequence_length, 1024)\n",
    "    prompt_embeddings = prompt_embeddings.to(device)\n",
    "\n",
    "\n",
    "target_text = \"It can be inferred that the audio is a recording of a musical performance or a rehearsal.The combination of music, speech,\"\\\n",
    "\"and sound effects suggests that the audio is a representation of a musical performance or a rehearsal, where the music is being played\"\\\n",
    "\"and the performers are practicing their performance or rehearsing.\"\n",
    "\n",
    "target_ids = tokenizer(target_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "decoder_input_ids = model._shift_right(target_ids)\n",
    "\n",
    "audio_embeddings = audio_input.to(device)  # Shape: (1, 32, 1024)\n",
    "\n",
    "# Concatenate prompt and audio embeddings\n",
    "combined_embeddings = torch.cat((prompt_embeddings, audio_embeddings), dim=1)  # Shape: (1, sequence_length + 32, 1024)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "\n",
    "if combined_embeddings.size(1) > max_length:\n",
    "    combined_embeddings = combined_embeddings[:, :max_length, :]\n",
    "\n",
    "padding_length = max_length - combined_embeddings.size(1)\n",
    "if padding_length > 0:\n",
    "    padding_tensor = torch.zeros((combined_embeddings.size(0), padding_length, combined_embeddings.size(2))).to(device)\n",
    "    combined_embeddings = torch.cat((combined_embeddings, padding_tensor), dim=1)\n",
    "\n",
    "\n",
    "attention_mask = torch.ones(combined_embeddings.size(0), combined_embeddings.size(1)).to(device)\n",
    "if padding_length > 0:\n",
    "    attention_mask[:, -padding_length:] = 0\n",
    "\n",
    "\n",
    "outputs = model(inputs_embeds=combined_embeddings, attention_mask=attention_mask,labels=decoder_input_ids)\n",
    "logits = outputs.logits  \n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "decoded_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623bfc9b-d511-404c-a4a3-3170a3991509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
