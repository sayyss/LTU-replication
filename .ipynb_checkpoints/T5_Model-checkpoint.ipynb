{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d904a05b-9a7d-4e55-b5e8-d94f69d0020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "import torchaudio\n",
    "from Cave_model import CAVMAEFTAudio\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb49928-847b-40a5-904e-3ef0234e2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3343f990-9a17-4126-8fd7-5a0d0eb041a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomT5ForConditionalGeneration(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.audio_encoder = CAVMAEFTAudio()\n",
    "        self.audio_proj = nn.Sequential(nn.LayerNorm(768, elementwise_affine=False), nn.Linear(768, 1024))\n",
    "\n",
    "        #nn.init.constant_(self.audio_proj[0].weight, 1)\n",
    "        #nn.init.constant_(self.audio_proj[0].bias, 0)\n",
    "        #nn.init.kaiming_normal_(self.audio_proj[1].weight, nonlinearity='relu')\n",
    "        #if self.audio_proj[1].bias is not None:\n",
    "        #    nn.init.constant_(self.audio_proj[1].bias, 0)\n",
    "                                                                                               \n",
    "        self.audio_encoder.initialize_weights()                                \n",
    "        self.post_init()\n",
    "    def forward(\n",
    "        self,\n",
    "        audio_input = None,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n",
    "            config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n",
    "            labels in `[0, ..., config.vocab_size]`\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "        >>> model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "        >>> # training\n",
    "        >>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "        >>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "        >>> outputs = model(input_ids=input_ids, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\n",
    "        >>> # inference\n",
    "        >>> input_ids = tokenizer(\n",
    "        ...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    "        ... ).input_ids  # Batch size 1\n",
    "        >>> outputs = model.generate(input_ids)\n",
    "        >>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        >>> # studies have shown that owning a dog is good for you.\n",
    "        ```\"\"\"\n",
    "        \n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
    "        if head_mask is not None and decoder_head_mask is None:\n",
    "            if self.config.num_layers == self.config.num_decoder_layers:\n",
    "                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n",
    "                decoder_head_mask = head_mask\n",
    "\n",
    "        # ******** Custom modifications start *********\n",
    "        # Get audio embeddings\n",
    "        if audio_input == None:\n",
    "            raise ValueError(\"audio input cannot be empty\")\n",
    "            \n",
    "        audio_input = self.audio_encoder(audio_input)  # [B, 512, 768]\n",
    "        audio_input = audio_input.reshape(audio_input.shape[0], 8, 64, audio_input.shape[-1])\n",
    "        audio_input = torch.mean(audio_input, dim=1)  # mean pool over the frequency dimension # [B, 64, 768]\n",
    "        audio_input = torch.nn.functional.avg_pool2d(audio_input, (2, 1)) #[B, 32, 768\n",
    "        # hard norm to 50\n",
    "        audio_input = audio_input / 50\n",
    "        audio_input = self.audio_proj(audio_input) #[B, 32, 1024]\n",
    "        audio_length = audio_input.shape[1]\n",
    "        \n",
    "        # Custom: get embeddings \n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.shared(input_ids).to(device)\n",
    "\n",
    "        print(\"Audio:\", audio_input)\n",
    "        print(\"input_embeds:\", inputs_embeds)\n",
    "        seq_length = audio_length + inputs_embeds.shape[1] # [32+seq_length]\n",
    "    \n",
    "        audio_embeds = audio_input.to(device) # [2,32,1024] \n",
    "        #print(audio_embeds.shape)\n",
    "        #print(inputs_embeds.shape) # shape: [2,seq_length, 1024]\n",
    "        inputs_embeds = torch.cat((inputs_embeds, audio_embeds), dim=1)  # Shape: (2,sequence_length + 32, 1024)\n",
    "        print(\"concat shape:\",inputs_embeds.shape) # shape: [2,seq+32, 1024]\n",
    "\n",
    "        max_length = 512\n",
    "        seq_length = inputs_embeds.size(1)  \n",
    "        padding_length = max_length - seq_length\n",
    "        \n",
    "        # Truncate if the sequence length exceeds max_length\n",
    "        if seq_length > max_length:\n",
    "            inputs_embeds = inputs_embeds[:, :max_length, :]\n",
    "        \n",
    "        # Apply padding if the sequence is shorter than max_length\n",
    "        if padding_length > 0:\n",
    "            padding_tensor = torch.zeros((inputs_embeds.size(0), padding_length, inputs_embeds.size(2))).to(device)\n",
    "            inputs_embeds = torch.cat((inputs_embeds, padding_tensor), dim=1) \n",
    "        \n",
    "\n",
    "        attention_mask = torch.ones((inputs_embeds.size(0), inputs_embeds.size(1))).to(device)  # Match sequence dimension\n",
    "\n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        if encoder_outputs is None:\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=None, # Custom: change to none because we already defined embeddings\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        hidden_states = encoder_outputs[0]\n",
    "\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "\n",
    "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            # get decoder inputs from shifting lm labels to the right\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "            hidden_states = hidden_states.to(self.decoder.first_device)\n",
    "            if decoder_input_ids is not None:\n",
    "                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.decoder.first_device)\n",
    "            if decoder_attention_mask is not None:\n",
    "                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = decoder_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.encoder.first_device)\n",
    "            self.lm_head = self.lm_head.to(self.encoder.first_device)\n",
    "            sequence_output = sequence_output.to(self.lm_head.weight.device)\n",
    "\n",
    "        if self.config.tie_word_embeddings:\n",
    "            # Rescale output before projecting on vocab\n",
    "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "            sequence_output = sequence_output * (self.model_dim**-0.5)\n",
    "\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            # move labels to correct device to enable PP\n",
    "            labels = labels.to(lm_logits.device)\n",
    "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e8489-edf1-4f6b-83e2-314e004d6f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f59470-b3b8-4e11-87e5-2c5291e33ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/sayyss/.conda/envs/LTU-Replication/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fill_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m T5Config\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize your custom model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m customT5 \u001b[38;5;241m=\u001b[39m CustomT5ForConditionalGeneration(config)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m customT5\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mCustomT5ForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_encoder \u001b[38;5;241m=\u001b[39m CAVMAEFTAudio()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;241m768\u001b[39m, elementwise_affine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m1024\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mconstant_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_proj[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mconstant_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_proj[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mkaiming_normal_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_proj[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mweight, nonlinearity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/init.py:221\u001b[0m, in \u001b[0;36mconstant_\u001b[0;34m(tensor, val)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(constant_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, val\u001b[38;5;241m=\u001b[39mval)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _no_grad_fill_(tensor, val)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/init.py:60\u001b[0m, in \u001b[0;36m_no_grad_fill_\u001b[0;34m(tensor, val)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_fill_\u001b[39m(tensor, val):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mfill_(val)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fill_'"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and config\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "config = T5Config.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# Initialize your custom model\n",
    "customT5 = CustomT5ForConditionalGeneration(config)\n",
    "model = customT5.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949d95ef-f972-48b1-be03-7d67a4164f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def get_methods_and_params(cls):\n",
    "    methods_and_params = []\n",
    "    for name, member in inspect.getmembers(cls):\n",
    "        if inspect.ismethod(member) or inspect.isfunction(member):\n",
    "            parameters = inspect.signature(member).parameters\n",
    "            param_names = [param for param in parameters.keys() if param != 'self']\n",
    "            methods_and_params.append((name, tuple(param_names)))\n",
    "    return methods_and_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60da6ba5-44dc-437c-9282-a944b0536f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayyss/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('__call__', ('args', 'kwargs')),\n",
       " ('__delattr__', ('name',)),\n",
       " ('__dir__', ()),\n",
       " ('__getattr__', ('name',)),\n",
       " ('__getstate__', ()),\n",
       " ('__init__', ('config',)),\n",
       " ('__repr__', ()),\n",
       " ('__setattr__', ('name', 'value')),\n",
       " ('__setstate__', ('state',)),\n",
       " ('_apply', ('fn', 'recurse')),\n",
       " ('_assisted_decoding',\n",
       "  ('input_ids',\n",
       "   'candidate_generator',\n",
       "   'logits_processor',\n",
       "   'logits_warper',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'streamer',\n",
       "   'model_kwargs')),\n",
       " ('_autoset_attn_implementation',\n",
       "  ('config',\n",
       "   'use_flash_attention_2',\n",
       "   'torch_dtype',\n",
       "   'device_map',\n",
       "   'check_device_map')),\n",
       " ('_backward_compatibility_gradient_checkpointing', ()),\n",
       " ('_beam_sample',\n",
       "  ('input_ids',\n",
       "   'beam_scorer',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'logits_warper',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'model_kwargs')),\n",
       " ('_beam_search',\n",
       "  ('input_ids',\n",
       "   'beam_scorer',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'logits_warper',\n",
       "   'model_kwargs')),\n",
       " ('_call_impl', ('args', 'kwargs')),\n",
       " ('_check_and_enable_flash_attn_2',\n",
       "  ('config',\n",
       "   'torch_dtype',\n",
       "   'device_map',\n",
       "   'check_device_map',\n",
       "   'hard_check_only')),\n",
       " ('_check_and_enable_sdpa', ('config', 'hard_check_only')),\n",
       " ('_constrained_beam_search',\n",
       "  ('input_ids',\n",
       "   'constrained_beam_scorer',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'model_kwargs')),\n",
       " ('_contrastive_search',\n",
       "  ('input_ids',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'streamer',\n",
       "   'model_kwargs')),\n",
       " ('_convert_head_mask_to_5d', ('head_mask', 'num_hidden_layers')),\n",
       " ('_copy_lm_head_original_to_resized',\n",
       "  ('new_lm_head',\n",
       "   'old_lm_head',\n",
       "   'num_tokens_to_copy',\n",
       "   'transposed',\n",
       "   'has_new_lm_head_bias')),\n",
       " ('_create_repo', ('repo_id', 'private', 'token', 'repo_url', 'organization')),\n",
       " ('_dispatch_accelerate_model',\n",
       "  ('device_map', 'max_memory', 'offload_folder', 'offload_index')),\n",
       " ('_expand_inputs_for_generation',\n",
       "  ('expand_size', 'is_encoder_decoder', 'input_ids', 'model_kwargs')),\n",
       " ('_extract_past_from_model_output', ('outputs', 'standardize_cache_format')),\n",
       " ('_from_config', ('config', 'kwargs')),\n",
       " ('_get_backward_hooks', ()),\n",
       " ('_get_backward_pre_hooks', ()),\n",
       " ('_get_candidate_generator',\n",
       "  ('generation_config',\n",
       "   'input_ids',\n",
       "   'inputs_tensor',\n",
       "   'assistant_model',\n",
       "   'logits_processor',\n",
       "   'model_kwargs')),\n",
       " ('_get_decoder_start_token_id', ('decoder_start_token_id', 'bos_token_id')),\n",
       " ('_get_files_timestamps', ('working_dir',)),\n",
       " ('_get_initial_cache_position', ('input_ids', 'model_kwargs')),\n",
       " ('_get_logits_processor',\n",
       "  ('generation_config',\n",
       "   'input_ids_seq_length',\n",
       "   'encoder_input_ids',\n",
       "   'prefix_allowed_tokens_fn',\n",
       "   'logits_processor',\n",
       "   'device',\n",
       "   'model_kwargs',\n",
       "   'negative_prompt_ids',\n",
       "   'negative_prompt_attention_mask')),\n",
       " ('_get_logits_warper', ('generation_config',)),\n",
       " ('_get_name', ()),\n",
       " ('_get_no_split_modules', ('device_map',)),\n",
       " ('_get_resized_embeddings',\n",
       "  ('old_embeddings', 'new_num_tokens', 'pad_to_multiple_of')),\n",
       " ('_get_resized_lm_head', ('old_lm_head', 'new_num_tokens', 'transposed')),\n",
       " ('_get_static_cache', ('max_batch_size', 'max_cache_len')),\n",
       " ('_get_stopping_criteria',\n",
       "  ('generation_config', 'stopping_criteria', 'tokenizer', 'kwargs')),\n",
       " ('_greedy_search',\n",
       "  ('input_ids',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'streamer',\n",
       "   'model_kwargs')),\n",
       " ('_group_beam_search',\n",
       "  ('input_ids',\n",
       "   'beam_scorer',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'model_kwargs')),\n",
       " ('_has_unfinished_sequences',\n",
       "  ('this_peer_finished', 'synced_gpus', 'device')),\n",
       " ('_hook_rss_memory_post_forward', ('module', 'args', 'kwargs')),\n",
       " ('_hook_rss_memory_pre_forward', ('module', 'args', 'kwargs')),\n",
       " ('_init_weights', ('module',)),\n",
       " ('_initialize_weights', ('module',)),\n",
       " ('_load_from_state_dict',\n",
       "  ('state_dict',\n",
       "   'prefix',\n",
       "   'local_metadata',\n",
       "   'strict',\n",
       "   'missing_keys',\n",
       "   'unexpected_keys',\n",
       "   'error_msgs')),\n",
       " ('_load_pretrained_model',\n",
       "  ('model',\n",
       "   'state_dict',\n",
       "   'loaded_keys',\n",
       "   'resolved_archive_file',\n",
       "   'pretrained_model_name_or_path',\n",
       "   'ignore_mismatched_sizes',\n",
       "   'sharded_metadata',\n",
       "   '_fast_init',\n",
       "   'low_cpu_mem_usage',\n",
       "   'device_map',\n",
       "   'offload_folder',\n",
       "   'offload_state_dict',\n",
       "   'dtype',\n",
       "   'hf_quantizer',\n",
       "   'keep_in_fp32_modules',\n",
       "   'gguf_path')),\n",
       " ('_load_pretrained_model_low_mem',\n",
       "  ('model',\n",
       "   'loaded_state_dict_keys',\n",
       "   'resolved_archive_file',\n",
       "   'start_prefix',\n",
       "   'hf_quantizer')),\n",
       " ('_maybe_initialize_input_ids_for_generation',\n",
       "  ('inputs', 'bos_token_id', 'model_kwargs')),\n",
       " ('_maybe_warn_non_full_backward_hook', ('inputs', 'result', 'grad_fn')),\n",
       " ('_merge_criteria_processor_list', ('default_list', 'custom_list')),\n",
       " ('_named_members',\n",
       "  ('get_members_fn', 'prefix', 'recurse', 'remove_duplicate')),\n",
       " ('_prepare_attention_mask_for_generation',\n",
       "  ('inputs', 'pad_token_id', 'eos_token_id')),\n",
       " ('_prepare_decoder_input_ids_for_generation',\n",
       "  ('batch_size',\n",
       "   'model_input_name',\n",
       "   'model_kwargs',\n",
       "   'decoder_start_token_id',\n",
       "   'device')),\n",
       " ('_prepare_encoder_decoder_kwargs_for_generation',\n",
       "  ('inputs_tensor', 'model_kwargs', 'model_input_name', 'generation_config')),\n",
       " ('_prepare_generated_length',\n",
       "  ('generation_config',\n",
       "   'has_default_max_length',\n",
       "   'has_default_min_length',\n",
       "   'model_input_name',\n",
       "   'input_ids_length',\n",
       "   'inputs_tensor')),\n",
       " ('_prepare_generation_config', ('generation_config', 'kwargs')),\n",
       " ('_prepare_model_inputs', ('inputs', 'bos_token_id', 'model_kwargs')),\n",
       " ('_prepare_special_tokens',\n",
       "  ('generation_config', 'kwargs_has_attention_mask', 'device')),\n",
       " ('_register_load_state_dict_pre_hook', ('hook', 'with_module')),\n",
       " ('_register_state_dict_hook', ('hook',)),\n",
       " ('_reorder_cache', ('past_key_values', 'beam_idx')),\n",
       " ('_replicate_for_data_parallel', ()),\n",
       " ('_resize_token_embeddings', ('new_num_tokens', 'pad_to_multiple_of')),\n",
       " ('_sample',\n",
       "  ('input_ids',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'generation_config',\n",
       "   'synced_gpus',\n",
       "   'streamer',\n",
       "   'logits_warper',\n",
       "   'model_kwargs')),\n",
       " ('_save_to_state_dict', ('destination', 'prefix', 'keep_vars')),\n",
       " ('_set_default_torch_dtype', ('dtype',)),\n",
       " ('_set_gradient_checkpointing', ('enable', 'gradient_checkpointing_func')),\n",
       " ('_shift_right', ('input_ids',)),\n",
       " ('_slow_forward', ('input', 'kwargs')),\n",
       " ('_temporary_reorder_cache', ('past_key_values', 'beam_idx')),\n",
       " ('_tie_encoder_decoder_weights',\n",
       "  ('encoder', 'decoder', 'base_model_prefix', 'base_encoder_name')),\n",
       " ('_tie_or_clone_weights', ('output_embeddings', 'input_embeddings')),\n",
       " ('_tie_weights', ()),\n",
       " ('_update_model_kwargs_for_generation',\n",
       "  ('outputs',\n",
       "   'model_kwargs',\n",
       "   'is_encoder_decoder',\n",
       "   'standardize_cache_format',\n",
       "   'num_new_tokens')),\n",
       " ('_upload_modified_files',\n",
       "  ('working_dir',\n",
       "   'repo_id',\n",
       "   'files_timestamps',\n",
       "   'commit_message',\n",
       "   'token',\n",
       "   'create_pr',\n",
       "   'revision',\n",
       "   'commit_description')),\n",
       " ('_validate_generated_length',\n",
       "  ('generation_config', 'input_ids_length', 'has_default_max_length')),\n",
       " ('_validate_model_class', ()),\n",
       " ('_validate_model_kwargs', ('model_kwargs',)),\n",
       " ('_wrapped_call_impl', ('args', 'kwargs')),\n",
       " ('active_adapter', ()),\n",
       " ('active_adapters', ()),\n",
       " ('add_adapter', ('adapter_config', 'adapter_name')),\n",
       " ('add_memory_hooks', ()),\n",
       " ('add_model_tags', ('tags',)),\n",
       " ('add_module', ('name', 'module')),\n",
       " ('apply', ('fn',)),\n",
       " ('bfloat16', ()),\n",
       " ('buffers', ('recurse',)),\n",
       " ('can_generate', ()),\n",
       " ('children', ()),\n",
       " ('compile', ('args', 'kwargs')),\n",
       " ('compute_transition_scores',\n",
       "  ('sequences', 'scores', 'beam_indices', 'normalize_logits')),\n",
       " ('cpu', ()),\n",
       " ('create_extended_attention_mask_for_decoder',\n",
       "  ('input_shape', 'attention_mask', 'device')),\n",
       " ('cuda', ('device',)),\n",
       " ('deparallelize', ()),\n",
       " ('dequantize', ()),\n",
       " ('disable_adapters', ()),\n",
       " ('disable_input_require_grads', ()),\n",
       " ('double', ()),\n",
       " ('enable_adapters', ()),\n",
       " ('enable_input_require_grads', ()),\n",
       " ('estimate_tokens', ('input_dict',)),\n",
       " ('eval', ()),\n",
       " ('extra_repr', ()),\n",
       " ('float', ('args',)),\n",
       " ('floating_point_ops', ('input_dict', 'exclude_embeddings')),\n",
       " ('forward',\n",
       "  ('audio_input',\n",
       "   'input_ids',\n",
       "   'attention_mask',\n",
       "   'decoder_input_ids',\n",
       "   'decoder_attention_mask',\n",
       "   'head_mask',\n",
       "   'decoder_head_mask',\n",
       "   'cross_attn_head_mask',\n",
       "   'encoder_outputs',\n",
       "   'past_key_values',\n",
       "   'inputs_embeds',\n",
       "   'decoder_inputs_embeds',\n",
       "   'labels',\n",
       "   'use_cache',\n",
       "   'output_attentions',\n",
       "   'output_hidden_states',\n",
       "   'return_dict')),\n",
       " ('from_pretrained',\n",
       "  ('pretrained_model_name_or_path',\n",
       "   'model_args',\n",
       "   'config',\n",
       "   'cache_dir',\n",
       "   'ignore_mismatched_sizes',\n",
       "   'force_download',\n",
       "   'local_files_only',\n",
       "   'token',\n",
       "   'revision',\n",
       "   'use_safetensors',\n",
       "   'kwargs')),\n",
       " ('generate',\n",
       "  ('inputs',\n",
       "   'generation_config',\n",
       "   'logits_processor',\n",
       "   'stopping_criteria',\n",
       "   'prefix_allowed_tokens_fn',\n",
       "   'synced_gpus',\n",
       "   'assistant_model',\n",
       "   'streamer',\n",
       "   'negative_prompt_ids',\n",
       "   'negative_prompt_attention_mask',\n",
       "   'kwargs')),\n",
       " ('get_adapter_state_dict', ('adapter_name',)),\n",
       " ('get_buffer', ('target',)),\n",
       " ('get_decoder', ()),\n",
       " ('get_encoder', ()),\n",
       " ('get_extended_attention_mask',\n",
       "  ('attention_mask', 'input_shape', 'device', 'dtype')),\n",
       " ('get_extra_state', ()),\n",
       " ('get_head_mask', ('head_mask', 'num_hidden_layers', 'is_attention_chunked')),\n",
       " ('get_input_embeddings', ()),\n",
       " ('get_memory_footprint', ('return_buffers',)),\n",
       " ('get_output_embeddings', ()),\n",
       " ('get_parameter', ('target',)),\n",
       " ('get_position_embeddings', ()),\n",
       " ('get_submodule', ('target',)),\n",
       " ('gradient_checkpointing_disable', ()),\n",
       " ('gradient_checkpointing_enable', ('gradient_checkpointing_kwargs',)),\n",
       " ('half', ('args',)),\n",
       " ('init_weights', ()),\n",
       " ('invert_attention_mask', ('encoder_attention_mask',)),\n",
       " ('ipu', ('device',)),\n",
       " ('load_adapter',\n",
       "  ('peft_model_id',\n",
       "   'adapter_name',\n",
       "   'revision',\n",
       "   'token',\n",
       "   'device_map',\n",
       "   'max_memory',\n",
       "   'offload_folder',\n",
       "   'offload_index',\n",
       "   'peft_config',\n",
       "   'adapter_state_dict',\n",
       "   'adapter_kwargs')),\n",
       " ('load_state_dict', ('state_dict', 'strict', 'assign')),\n",
       " ('load_tf_weights', ('config', 'tf_checkpoint_path')),\n",
       " ('modules', ()),\n",
       " ('named_buffers', ('prefix', 'recurse', 'remove_duplicate')),\n",
       " ('named_children', ()),\n",
       " ('named_modules', ('memo', 'prefix', 'remove_duplicate')),\n",
       " ('named_parameters', ('prefix', 'recurse', 'remove_duplicate')),\n",
       " ('num_parameters', ('only_trainable', 'exclude_embeddings')),\n",
       " ('parallelize', ('device_map',)),\n",
       " ('parameters', ('recurse',)),\n",
       " ('post_init', ()),\n",
       " ('prepare_decoder_input_ids_from_labels', ('labels',)),\n",
       " ('prepare_inputs_for_generation',\n",
       "  ('input_ids',\n",
       "   'past_key_values',\n",
       "   'attention_mask',\n",
       "   'head_mask',\n",
       "   'decoder_head_mask',\n",
       "   'decoder_attention_mask',\n",
       "   'cross_attn_head_mask',\n",
       "   'use_cache',\n",
       "   'encoder_outputs',\n",
       "   'kwargs')),\n",
       " ('prune_heads', ('heads_to_prune',)),\n",
       " ('push_to_hub',\n",
       "  ('repo_id',\n",
       "   'use_temp_dir',\n",
       "   'commit_message',\n",
       "   'private',\n",
       "   'token',\n",
       "   'max_shard_size',\n",
       "   'create_pr',\n",
       "   'safe_serialization',\n",
       "   'revision',\n",
       "   'commit_description',\n",
       "   'tags',\n",
       "   'deprecated_kwargs')),\n",
       " ('register_backward_hook', ('hook',)),\n",
       " ('register_buffer', ('name', 'tensor', 'persistent')),\n",
       " ('register_for_auto_class', ('auto_class',)),\n",
       " ('register_forward_hook', ('hook', 'prepend', 'with_kwargs', 'always_call')),\n",
       " ('register_forward_pre_hook', ('hook', 'prepend', 'with_kwargs')),\n",
       " ('register_full_backward_hook', ('hook', 'prepend')),\n",
       " ('register_full_backward_pre_hook', ('hook', 'prepend')),\n",
       " ('register_load_state_dict_post_hook', ('hook',)),\n",
       " ('register_module', ('name', 'module')),\n",
       " ('register_parameter', ('name', 'param')),\n",
       " ('register_state_dict_pre_hook', ('hook',)),\n",
       " ('requires_grad_', ('requires_grad',)),\n",
       " ('reset_memory_hooks_state', ()),\n",
       " ('resize_position_embeddings', ('new_num_position_embeddings',)),\n",
       " ('resize_token_embeddings', ('new_num_tokens', 'pad_to_multiple_of')),\n",
       " ('retrieve_modules_from_names', ('names', 'add_prefix', 'remove_prefix')),\n",
       " ('reverse_bettertransformer', ()),\n",
       " ('save_pretrained',\n",
       "  ('save_directory',\n",
       "   'is_main_process',\n",
       "   'state_dict',\n",
       "   'save_function',\n",
       "   'push_to_hub',\n",
       "   'max_shard_size',\n",
       "   'safe_serialization',\n",
       "   'variant',\n",
       "   'token',\n",
       "   'save_peft_format',\n",
       "   'kwargs')),\n",
       " ('set_adapter', ('adapter_name',)),\n",
       " ('set_extra_state', ('state',)),\n",
       " ('set_input_embeddings', ('new_embeddings',)),\n",
       " ('set_output_embeddings', ('new_embeddings',)),\n",
       " ('share_memory', ()),\n",
       " ('state_dict', ('args', 'destination', 'prefix', 'keep_vars')),\n",
       " ('tie_weights', ()),\n",
       " ('to', ('args', 'kwargs')),\n",
       " ('to_bettertransformer', ()),\n",
       " ('to_empty', ('device', 'recurse')),\n",
       " ('train', ('mode',)),\n",
       " ('type', ('dst_type',)),\n",
       " ('warn_if_padding_and_no_attention_mask', ('input_ids', 'attention_mask')),\n",
       " ('xpu', ('device',)),\n",
       " ('zero_grad', ('set_to_none',))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_methods_and_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c06a6c2-48ce-4c66-b0c1-8eddebb2d8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869658624"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2146e1-efdc-48a7-a084-09b3d379b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./data/toy_dataset/openaqa_toy.json\"\n",
    "\n",
    "with open(file, \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c435d-eafe-4f64-9f28-6dd0e5c8f16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116d9065-5f15-4d5a-8178-9dd6d3f4b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/toy_dataset/audio/4tnW9atZKo0.flac\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['audio_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff4e5abb-b769-4452-aeb9-804142cc8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    path = data[i]['audio_id']\n",
    "    exten = path[len(path)-4:]\n",
    "    if exten == \"flac\" or exten == \".wav\":\n",
    "        mini_path = \"\"\n",
    "        for j in range(len(path)-1, -1, -1):\n",
    "            if path[j] == \"/\":\n",
    "                break\n",
    "            mini_path += path[j]\n",
    "        data[i]['audio_id'] = \"./data/toy_dataset/audio/\" + mini_path[::-1]\n",
    "        \n",
    "with open(file, \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67caa384-5ded-4ca9-a2bf-23e36042eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=file, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "035c3b52-b557-4d9a-88b5-720125da5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ebd258-ca28-4e60-8659-e1d12fd45e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5044, 6), (1262, 6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape, test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039bef0a-76ac-412a-9ee3-31bc437727d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What could be some possible reasons why people would boo in an event?',\n",
       " 'input': '',\n",
       " 'audio_id': './data/toy_dataset/audio/-Tp-AspPvmg_000538.flac',\n",
       " 'dataset': 'vggsound_train',\n",
       " 'task': 'open-ended question',\n",
       " 'output': 'People could be booing due to a controversial decision made by a referee or judge, dissatisfaction with a political speech or decision, or a poor performance from an artist or athlete.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09c4c270-b7fc-4f13-a362-84f0e23ee5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filterbank\n",
    "def load_audio(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    audio_info = 'Original input audio length {:.2f} seconds, number of channels: {:d}, sampling rate: {:d}.'.format(waveform.shape[1]/sample_rate, waveform.shape[0], sample_rate)\n",
    "    if waveform.shape[0] != 1:\n",
    "        waveform = waveform[0].unsqueeze(0)\n",
    "        audio_info += ' Only the first channel is used.'\n",
    "    if sample_rate == 16000:\n",
    "        pass\n",
    "    else:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=sample_rate, new_freq=16000)\n",
    "        sample_rate = 16000\n",
    "        audio_info += ' Resample to 16000Hz.'\n",
    "    waveform = waveform - waveform.mean()\n",
    "    fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sample_rate,\n",
    "                                              use_energy=False, window_type='hanning',\n",
    "                                              num_mel_bins=128, dither=0.0, frame_shift=10)\n",
    "    target_length = 1024\n",
    "    n_frames = fbank.shape[0]\n",
    "    p = target_length - n_frames\n",
    "    if p > 0:\n",
    "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "        fbank = m(fbank)\n",
    "    elif p < 0:\n",
    "        fbank = fbank[0:target_length, :]\n",
    "    # normalize the fbank\n",
    "    fbank = (fbank + 5.081) / 4.4849\n",
    "    return fbank, audio_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "726a253e-ba72-41a3-94d3-e46f95459c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def return_audio(path):\n",
    "\n",
    "    cur_audio_input, audio_info = load_audio(path)\n",
    "    cur_audio_input = cur_audio_input.unsqueeze(0)\n",
    "    \n",
    "    # projecting to 1024 input embedding dimension for T5\n",
    "    audio_proj = nn.Sequential(nn.LayerNorm(768, elementwise_affine=False), nn.Linear(768, 1024))\n",
    "    audio_input = audio_encoder(cur_audio_input)  # [B, 512, 768]\n",
    "    audio_input = audio_input.reshape(audio_input.shape[0], 8, 64, audio_input.shape[-1])\n",
    "    audio_input = torch.mean(audio_input, dim=1)  # mean pool over the frequency dimension # [B, 64, 768]\n",
    "    audio_input = torch.nn.functional.avg_pool2d(audio_input, (2, 1)) #[B, 32, 768]\n",
    "    # hard norm to 50\n",
    "    audio_input = audio_input / 50\n",
    "    audio_input = audio_proj(audio_input) #[B, 32, 1024]\n",
    "    \n",
    "    return audio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c664636-70a2-44d0-9451-b6a8c321ecf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abfcde6e-8a70-4175-8b7e-3fe3c7149121",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42670ff5-a524-4846-9f6b-c93b944ed181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CombinedEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, model, device, max_length=512):\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instruction = self.data[idx]['instruction']\n",
    "        audio_path = self.data[idx]['audio_id']\n",
    "        label = self.data[idx]['output']\n",
    "        \n",
    "        input_ids = self.tokenizer(instruction, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        decoder_input_ids = self.tokenizer(label, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        audio_bank, audio_info = load_audio(audio_path)\n",
    "        audio_bank = audio_bank.to(self.device)\n",
    "        print(\"from dataloader:\", audio_bank.shape)\n",
    "        return input_ids, decoder_input_ids.squeeze(0), audio_bank\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    input_ids = [item[0].squeeze(0) for item in batch]  # Remove unnecessary dim\n",
    "    decoder_input_ids = [item[1] for item in batch]\n",
    "    audio_input = [item[2] for item in batch]\n",
    "\n",
    "    # Pad input_ids and decoder_input_ids to the maximum length in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    decoder_input_ids_padded = pad_sequence(decoder_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Stack audio inputs directly if they have the same size\n",
    "    audio_input_stacked = torch.stack(audio_input)\n",
    "    return input_ids_padded, audio_input_stacked, decoder_input_ids_padded\n",
    "\n",
    "\n",
    "dataset = CombinedEmbeddingsDataset(train_dataset, tokenizer, model, device)\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d6e7478-a346-430f-b431-1ad6ece42a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from dataloader: torch.Size([1024, 128])\n",
      "from loop: torch.Size([1, 22])\n",
      "from loop audio: torch.Size([1, 1024, 128])\n",
      "audio_bank: tensor([[[-0.8093, -1.5818, -0.8140,  ...,  1.4905,  1.0951,  1.1792],\n",
      "         [-1.1993, -1.7439, -0.9761,  ...,  1.5396,  1.3862,  1.0761],\n",
      "         [-0.5940, -1.6130, -0.8452,  ...,  1.3831,  1.1852,  1.0818],\n",
      "         ...,\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329]]],\n",
      "       device='cuda:0')\n",
      "from model input tensor([[[-0.8093, -1.5818, -0.8140,  ...,  1.4905,  1.0951,  1.1792],\n",
      "         [-1.1993, -1.7439, -0.9761,  ...,  1.5396,  1.3862,  1.0761],\n",
      "         [-0.5940, -1.6130, -0.8452,  ...,  1.3831,  1.1852,  1.0818],\n",
      "         ...,\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329]]],\n",
      "       device='cuda:0')\n",
      "from CAVE inside: tensor([[[-0.8093, -1.5818, -0.8140,  ...,  1.4905,  1.0951,  1.1792],\n",
      "         [-1.1993, -1.7439, -0.9761,  ...,  1.5396,  1.3862,  1.0761],\n",
      "         [-0.5940, -1.6130, -0.8452,  ...,  1.3831,  1.1852,  1.0818],\n",
      "         ...,\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329]]],\n",
      "       device='cuda:0')\n",
      "from CAVE before patch: tensor([[[[-0.8093, -1.1993, -0.5940,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5818, -1.7439, -1.6130,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-0.8140, -0.9761, -0.8452,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [ 1.4905,  1.5396,  1.3831,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [ 1.0951,  1.3862,  1.1852,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [ 1.1792,  1.0761,  1.0818,  ...,  1.1329,  1.1329,  1.1329]]]],\n",
      "       device='cuda:0')\n",
      "patch embed: torch.Size([1, 1, 128, 1024])\n",
      "from CAVE after patch: tensor([[[-1.8153e-35,  1.2235e+15, -5.4627e-36,  ...,  4.3562e-41,\n",
      "           3.0153e-37,  4.3562e-41],\n",
      "         [-1.3051e-35,  1.3828e+15, -1.3758e-36,  ...,  4.3562e-41,\n",
      "           3.0153e-37,  4.3562e-41],\n",
      "         [-9.6820e-36,  7.8059e+14, -9.6865e-37,  ...,  4.3562e-41,\n",
      "           3.0153e-37,  4.3562e-41],\n",
      "         ...,\n",
      "         [ 5.2552e-36,  3.2242e+15,  4.9093e-36,  ...,  4.3562e-41,\n",
      "           3.0153e-37,  4.3562e-41],\n",
      "         [ 3.2230e-35,  4.5089e+15,  2.5873e-35,  ...,  4.3562e-41,\n",
      "           3.0153e-37,  4.3562e-41],\n",
      "         [ 5.6578e-35,  7.0620e+15,  4.4345e-35,  ...,  4.3562e-41,\n",
      "           3.0153e-37,  4.3562e-41]]], device='cuda:0',\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "from CAVE inside 2: tensor([[[-1.6593e-35,  1.2235e+15, -3.9028e-36,  ...,  1.3069e-40,\n",
      "           9.0458e-37,  1.3069e-40],\n",
      "         [-1.1969e-35,  1.3828e+15, -2.9432e-37,  ...,  1.3069e-40,\n",
      "           8.6418e-37,  1.3069e-40],\n",
      "         [-8.5964e-36,  7.8059e+14,  7.2465e-38,  ...,  8.7124e-41,\n",
      "           6.0305e-37,  8.7124e-41],\n",
      "         ...,\n",
      "         [ 6.0352e-36,  3.2242e+15,  5.6893e-36,  ...,  8.7124e-41,\n",
      "           6.0305e-37,  8.7124e-41],\n",
      "         [ 3.3010e-35,  4.5089e+15,  2.6653e-35,  ...,  8.7124e-41,\n",
      "           6.0305e-37,  8.7124e-41],\n",
      "         [ 5.7358e-35,  7.0620e+15,  4.5125e-35,  ...,  8.7124e-41,\n",
      "           6.0305e-37,  8.7124e-41]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "shape before putting in block: torch.Size([1, 512, 768])\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "from CAVE inside 3: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "from CAVE inside 4: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "from CAVE: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "from reshape: tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "from mean: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "from pooling: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<AvgPool2DBackward0>)\n",
      "from norm: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "from proj: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Audio: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "input_embeds: tensor([[[ 3.2234,  1.9547, -7.2779,  ...,  0.8901, -4.7579, -2.5585],\n",
      "         [ 1.8323,  3.2133,  0.6548,  ..., -4.0022, -6.1845,  2.6286],\n",
      "         [-6.6324,  0.3264,  2.5050,  ..., -0.0821,  1.7839,  4.3345],\n",
      "         ...,\n",
      "         [-5.0613, 10.7229, -3.9151,  ..., -5.3143, 11.7969, 10.5836],\n",
      "         [10.1411, -2.0378,  2.5116,  ...,  0.1963, -0.3747,  0.1713],\n",
      "         [-2.7021, -2.1235,  0.5805,  ..., -2.3538, -4.8025, -5.8202]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "concat shape: torch.Size([1, 54, 1024])\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# Example labels (target sequence for training)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Iterate through the DataLoader\n",
    "    for i, (input_ids, audio_bank, decoder_input_ids) in enumerate(itertools.islice(dataloader, 1)):\n",
    "        if torch.isnan(input_ids).any() or torch.isnan(audio_bank).any() or torch.isnan(decoder_input_ids).any():\n",
    "            print(\"NaN detected in input tensors\")\n",
    "        print(\"from loop:\",input_ids.shape)\n",
    "        print(\"from loop audio:\", audio_bank.shape)\n",
    "        optimizer.zero_grad()\n",
    "        print(\"audio_bank:\",audio_bank)\n",
    "        outputs = model(input_ids=input_ids, audio_input=audio_bank,labels=decoder_input_ids)\n",
    "       #print(outputs)\n",
    "        loss = outputs.loss\n",
    "        print(loss.item())\n",
    "        #logits = outputs.logits  \n",
    "        #predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        #decoded_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        #print(decoded_text)\n",
    "        #predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70b4ebe8-5573-455b-8fe1-c888b007921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, \"first_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad61e18f-41af-4dcd-8fed-676f33666e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "checkpoint = torch.load(\"first_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10efe568-3e31-4845-af27-f422fd7e0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cave_model import CAVMAEFTAudio\n",
    "audio_encoder = CAVMAEFTAudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c1a5635-50ab-4e71-8fb5-4b2c1355ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from CAVE inside: tensor([[[-2.2843, -2.4218, -2.0455,  ..., -0.9920, -1.5722, -2.3284],\n",
      "         [-1.8898, -2.3895, -1.6217,  ..., -0.2275, -1.0407, -2.1476],\n",
      "         [-1.6393, -2.1380, -1.3702,  ...,  0.0030, -0.7107, -1.5814],\n",
      "         ...,\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329]]])\n",
      "from CAVE before patch: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]])\n",
      "patch embed: torch.Size([1, 1, 128, 1024])\n",
      "before convo: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]])\n",
      "from CAVE after patch: tensor([[[ 0.4362,  0.4526,  1.0895,  ...,  0.4306,  0.2157, -0.4852],\n",
      "         [ 0.5653,  0.4318,  1.5278,  ...,  0.7709,  0.5457, -0.2353],\n",
      "         [ 0.7882,  0.6527,  1.2280,  ...,  0.4815,  0.6577, -0.4015],\n",
      "         ...,\n",
      "         [-0.5898, -0.3371, -0.7685,  ..., -0.3628, -0.4055,  0.2191],\n",
      "         [-0.5898, -0.3371, -0.7685,  ..., -0.3628, -0.4055,  0.2191],\n",
      "         [-0.5898, -0.3371, -0.7685,  ..., -0.3628, -0.4055,  0.2191]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "from CAVE inside 2: tensor([[[ 0.4213,  0.4086,  1.1101,  ...,  1.3810,  1.2224,  0.4998],\n",
      "         [ 1.3918,  1.2031,  2.3370,  ...,  1.7214,  1.5525,  0.7497],\n",
      "         [ 1.6826,  1.5529,  2.2184,  ...,  1.4319,  1.6645,  0.5835],\n",
      "         ...,\n",
      "         [-1.5709,  0.6186, -1.6520,  ...,  0.5876,  0.6013,  1.2041],\n",
      "         [-1.3440,  0.1787, -0.9667,  ...,  0.5876,  0.6013,  1.2041],\n",
      "         [-0.4374, -0.7324, -0.1130,  ...,  0.5876,  0.6013,  1.2041]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "shape before putting in block: torch.Size([1, 512, 768])\n",
      "from CAVE inside 3: tensor([[[ 1.7817, -0.8335, -0.9296,  ...,  1.1778, -1.3637, -1.0768],\n",
      "         [ 1.8061, -0.8047, -0.8683,  ...,  1.1784, -1.4363, -1.0452],\n",
      "         [ 1.8128, -0.8327, -0.8677,  ...,  1.1886, -1.4213, -1.0684],\n",
      "         ...,\n",
      "         [ 1.6497, -0.4733, -1.0248,  ...,  1.0663, -1.1578, -0.9117],\n",
      "         [ 1.6911, -0.4805, -0.9718,  ...,  1.0692, -1.1771, -0.9116],\n",
      "         [ 1.7563, -0.5001, -0.9225,  ...,  1.0600, -1.2031, -0.9054]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "from CAVE inside 4: tensor([[[ 1.5250, -0.7497, -0.8334,  ...,  0.9997, -1.2109, -0.9614],\n",
      "         [ 1.5444, -0.7245, -0.7798,  ...,  0.9989, -1.2734, -0.9335],\n",
      "         [ 1.5501, -0.7484, -0.7788,  ...,  1.0077, -1.2598, -0.9532],\n",
      "         ...,\n",
      "         [ 1.3932, -0.4303, -0.9040,  ...,  0.8921, -1.0183, -0.8069],\n",
      "         [ 1.4290, -0.4366, -0.8588,  ...,  0.8948, -1.0352, -0.8070],\n",
      "         [ 1.4856, -0.4537, -0.8167,  ...,  0.8871, -1.0579, -0.8020]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5250, -0.7497, -0.8334,  ...,  0.9997, -1.2109, -0.9614],\n",
       "         [ 1.5444, -0.7245, -0.7798,  ...,  0.9989, -1.2734, -0.9335],\n",
       "         [ 1.5501, -0.7484, -0.7788,  ...,  1.0077, -1.2598, -0.9532],\n",
       "         ...,\n",
       "         [ 1.3932, -0.4303, -0.9040,  ...,  0.8921, -1.0183, -0.8069],\n",
       "         [ 1.4290, -0.4366, -0.8588,  ...,  0.8948, -1.0352, -0.8070],\n",
       "         [ 1.4856, -0.4537, -0.8167,  ...,  0.8871, -1.0579, -0.8020]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"./data/toy_dataset/audio/577026.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0)\n",
    "audio_input = audio_encoder(cur_audio_input) \n",
    "audio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4241216e-6a36-4ead-b4c7-1b3f7fc434f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from model input tensor([[[-2.2843, -2.4218, -2.0455,  ..., -0.9920, -1.5722, -2.3284],\n",
      "         [-1.8898, -2.3895, -1.6217,  ..., -0.2275, -1.0407, -2.1476],\n",
      "         [-1.6393, -2.1380, -1.3702,  ...,  0.0030, -0.7107, -1.5814],\n",
      "         ...,\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329]]],\n",
      "       device='cuda:0')\n",
      "from CAVE inside: tensor([[[-2.2843, -2.4218, -2.0455,  ..., -0.9920, -1.5722, -2.3284],\n",
      "         [-1.8898, -2.3895, -1.6217,  ..., -0.2275, -1.0407, -2.1476],\n",
      "         [-1.6393, -2.1380, -1.3702,  ...,  0.0030, -0.7107, -1.5814],\n",
      "         ...,\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329],\n",
      "         [ 1.1329,  1.1329,  1.1329,  ...,  1.1329,  1.1329,  1.1329]]],\n",
      "       device='cuda:0')\n",
      "from CAVE before patch: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]],\n",
      "       device='cuda:0')\n",
      "patch embed: torch.Size([1, 1, 128, 1024])\n",
      "before convo: tensor([[[[-2.2843, -1.8898, -1.6393,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.4218, -2.3895, -2.1380,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.0455, -1.6217, -1.3702,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          ...,\n",
      "          [-0.9920, -0.2275,  0.0030,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-1.5722, -1.0407, -0.7107,  ...,  1.1329,  1.1329,  1.1329],\n",
      "          [-2.3284, -2.1476, -1.5814,  ...,  1.1329,  1.1329,  1.1329]]]],\n",
      "       device='cuda:0')\n",
      "from CAVE after patch: tensor([[[-1.3978e-29, -5.9123e+24, -1.5671e-29,  ...,  4.3153e-41,\n",
      "           7.3683e-32,  4.3153e-41],\n",
      "         [-1.5201e-29, -6.0932e+24, -1.7261e-29,  ...,  4.3153e-41,\n",
      "           7.3683e-32,  4.3153e-41],\n",
      "         [-1.8058e-29, -8.1505e+24, -2.0376e-29,  ...,  4.3153e-41,\n",
      "           7.3683e-32,  4.3153e-41],\n",
      "         ...,\n",
      "         [ 9.4629e-30,  3.8129e+24,  1.0603e-29,  ...,  4.3153e-41,\n",
      "           7.3683e-32,  4.3153e-41],\n",
      "         [ 9.4629e-30,  3.8129e+24,  1.0603e-29,  ...,  4.3153e-41,\n",
      "           7.3683e-32,  4.3153e-41],\n",
      "         [ 9.4629e-30,  3.8129e+24,  1.0603e-29,  ...,  4.3153e-41,\n",
      "           7.3683e-32,  4.3153e-41]]], device='cuda:0',\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "from CAVE inside 2: tensor([[[-1.3949e-29, -5.9123e+24, -1.5642e-29,  ...,  1.2946e-40,\n",
      "           2.2105e-31,  1.2946e-40],\n",
      "         [-1.5113e-29, -6.0932e+24, -1.7173e-29,  ...,  1.2946e-40,\n",
      "           1.9419e-31,  1.2946e-40],\n",
      "         [-1.7997e-29, -8.1505e+24, -2.0314e-29,  ...,  8.6306e-41,\n",
      "           1.4737e-31,  8.6306e-41],\n",
      "         ...,\n",
      "         [ 9.4778e-30,  3.8129e+24,  1.0618e-29,  ...,  8.6306e-41,\n",
      "           1.4737e-31,  8.6306e-41],\n",
      "         [ 9.4778e-30,  3.8129e+24,  1.0618e-29,  ...,  8.6306e-41,\n",
      "           1.4737e-31,  8.6306e-41],\n",
      "         [ 9.4778e-30,  3.8129e+24,  1.0618e-29,  ...,  8.6306e-41,\n",
      "           1.4737e-31,  8.6306e-41]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "shape before putting in block: torch.Size([1, 512, 768])\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "NaN detected after norm1\n",
      "NaN detected after attn\n",
      "NaN detected after norm2\n",
      "NaN detected after mlp\n",
      "from CAVE inside 3: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "from CAVE inside 4: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "from CAVE: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "from reshape: tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "from mean: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "from pooling: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<AvgPool2DBackward0>)\n",
      "from norm: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "from proj: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Audio: tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "input_embeds: tensor([[[ 7.5833, -5.9968, -8.7719,  ..., -1.7548, -1.4999,  0.8449],\n",
      "         [ 8.0296,  3.5735, -0.2749,  ..., -0.3696,  0.0123, -2.4261],\n",
      "         [-7.9453, -3.8064,  5.7910,  ...,  2.7917,  1.0186,  1.6824],\n",
      "         ...,\n",
      "         [-7.1989, -7.0903, 10.7553,  ...,  7.1836,  5.0034, -9.7338],\n",
      "         [ 1.5147,  3.0822, -0.0883,  ...,  0.4944,  3.2871, -6.7017],\n",
      "         [-2.7021, -2.1235,  0.5805,  ..., -2.3538, -4.8025, -5.8202]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "concat shape: torch.Size([1, 43, 1024])\n"
     ]
    }
   ],
   "source": [
    "file = \"./data/toy_dataset/audio/577026.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0).to(device)\n",
    "prompt_text = \"what can be infered from this audio following\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "target_text = \"It can be inferred that the audio is a recording of a musical performance or a rehearsal.The combination of music, speech,\"\\\n",
    "\"and sound effects suggests that the audio is a representation of a musical performance or a rehearsal, where the music is being played\"\\\n",
    "\"and the performers are practicing their performance or rehearsing.\"\n",
    "target_ids = tokenizer(target_text, return_tensors='pt').input_ids.to(device)\n",
    "decoder_input_ids = model._shift_right(target_ids).to(device)\n",
    "\n",
    "outputs = model(input_ids=input_ids, audio_input=cur_audio_input,labels=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff4d13-16a4-40af-8066-5e834b39614a",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d429a4ed-867c-4a2c-8cb3-22beb5a44985",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "audio input cannot be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m     attention_mask[:, \u001b[38;5;241m-\u001b[39mpadding_length:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs_embeds\u001b[38;5;241m=\u001b[39mcombined_embeddings, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,labels\u001b[38;5;241m=\u001b[39mdecoder_input_ids)\n\u001b[1;32m     42\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits  \n\u001b[1;32m     44\u001b[0m predicted_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/LTU-Replication/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[65], line 82\u001b[0m, in \u001b[0;36mCustomT5ForConditionalGeneration.forward\u001b[0;34m(self, audio_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     80\u001b[0m     audio_length \u001b[38;5;241m=\u001b[39m audio_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio input cannot be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Custom: get embeddings \u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: audio input cannot be empty"
     ]
    }
   ],
   "source": [
    "audio_encoder = CAVMAEFTAudio()\n",
    "prompt_text = \"what can be infered from this audio following\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "audio_input = return_audio(\"./data/toy_dataset/audio/_4X8RNeWeDI.flac\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_embeddings = model.shared(input_ids)  # Shape: (1, sequence_length, 1024)\n",
    "    prompt_embeddings = prompt_embeddings.to(device)\n",
    "\n",
    "\n",
    "target_text = \"It can be inferred that the audio is a recording of a musical performance or a rehearsal.The combination of music, speech,\"\\\n",
    "\"and sound effects suggests that the audio is a representation of a musical performance or a rehearsal, where the music is being played\"\\\n",
    "\"and the performers are practicing their performance or rehearsing.\"\n",
    "\n",
    "target_ids = tokenizer(target_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "decoder_input_ids = model._shift_right(target_ids)\n",
    "\n",
    "audio_embeddings = audio_input.to(device)  # Shape: (1, 32, 1024)\n",
    "\n",
    "# Concatenate prompt and audio embeddings\n",
    "combined_embeddings = torch.cat((prompt_embeddings, audio_embeddings), dim=1)  # Shape: (1, sequence_length + 32, 1024)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "\n",
    "if combined_embeddings.size(1) > max_length:\n",
    "    combined_embeddings = combined_embeddings[:, :max_length, :]\n",
    "\n",
    "padding_length = max_length - combined_embeddings.size(1)\n",
    "if padding_length > 0:\n",
    "    padding_tensor = torch.zeros((combined_embeddings.size(0), padding_length, combined_embeddings.size(2))).to(device)\n",
    "    combined_embeddings = torch.cat((combined_embeddings, padding_tensor), dim=1)\n",
    "\n",
    "\n",
    "attention_mask = torch.ones(combined_embeddings.size(0), combined_embeddings.size(1)).to(device)\n",
    "if padding_length > 0:\n",
    "    attention_mask[:, -padding_length:] = 0\n",
    "\n",
    "\n",
    "outputs = model(inputs_embeds=combined_embeddings, attention_mask=attention_mask,labels=decoder_input_ids)\n",
    "logits = outputs.logits  \n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "decoded_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623bfc9b-d511-404c-a4a3-3170a3991509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
