{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d904a05b-9a7d-4e55-b5e8-d94f69d0020c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'T5Tokenizer' from 'transformers' (/home/sayyss/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, T5ForConditionalGeneration\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCave_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CAVMAEFTAudio\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'T5Tokenizer' from 'transformers' (/home/sayyss/.conda/envs/LTU-Replication/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torchaudio\n",
    "from Cave_model import CAVMAEFTAudio\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe09379-ee7a-4503-85ee-13bc6de1472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2146e1-efdc-48a7-a084-09b3d379b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./data/toy_dataset/openaqa_toy.json\"\n",
    "\n",
    "with open(file, \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c435d-eafe-4f64-9f28-6dd0e5c8f16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116d9065-5f15-4d5a-8178-9dd6d3f4b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/toy_dataset/audio/4tnW9atZKo0.flac\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['audio_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4e5abb-b769-4452-aeb9-804142cc8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    path = data[i]['audio_id']\n",
    "    exten = path[len(path)-4:]\n",
    "    if exten == \"flac\" or exten == \".wav\":\n",
    "        mini_path = \"\"\n",
    "        for j in range(len(path)-1, -1, -1):\n",
    "            if path[j] == \"/\":\n",
    "                break\n",
    "            mini_path += path[j]\n",
    "        data[i]['audio_id'] = \"./data/toy_dataset/audio/\" + mini_path[::-1]\n",
    "        \n",
    "with open(file, \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67caa384-5ded-4ca9-a2bf-23e36042eb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bd9c56223d4d3891d99ab6fb585fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=file, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "035c3b52-b557-4d9a-88b5-720125da5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ebd258-ca28-4e60-8659-e1d12fd45e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5044, 6), (1262, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape, test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "039bef0a-76ac-412a-9ee3-31bc437727d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is the mood conveyed by this audio clip?',\n",
       " 'audio_id': './data/toy_dataset/audio/WI_76zVEsLU_000295.flac',\n",
       " 'dataset': 'vggsound_train',\n",
       " 'task': 'open-ended question',\n",
       " 'input': '',\n",
       " 'output': \"The mood conveyed by this audio clip is soothing and calming, which is similar to a cat's purring.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09c4c270-b7fc-4f13-a362-84f0e23ee5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filterbank\n",
    "def load_audio(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    audio_info = 'Original input audio length {:.2f} seconds, number of channels: {:d}, sampling rate: {:d}.'.format(waveform.shape[1]/sample_rate, waveform.shape[0], sample_rate)\n",
    "    if waveform.shape[0] != 1:\n",
    "        waveform = waveform[0].unsqueeze(0)\n",
    "        audio_info += ' Only the first channel is used.'\n",
    "    if sample_rate == 16000:\n",
    "        pass\n",
    "    else:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=sample_rate, new_freq=16000)\n",
    "        sample_rate = 16000\n",
    "        audio_info += ' Resample to 16000Hz.'\n",
    "    waveform = waveform - waveform.mean()\n",
    "    fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sample_rate,\n",
    "                                              use_energy=False, window_type='hanning',\n",
    "                                              num_mel_bins=128, dither=0.0, frame_shift=10)\n",
    "    target_length = 1024\n",
    "    n_frames = fbank.shape[0]\n",
    "    p = target_length - n_frames\n",
    "    if p > 0:\n",
    "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "        fbank = m(fbank)\n",
    "    elif p < 0:\n",
    "        fbank = fbank[0:target_length, :]\n",
    "    # normalize the fbank\n",
    "    fbank = (fbank + 5.081) / 4.4849\n",
    "    return fbank, audio_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726a253e-ba72-41a3-94d3-e46f95459c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_encoder = CAVMAEFTAudio()\n",
    "\n",
    "def return_audio(path):\n",
    "\n",
    "    cur_audio_input, audio_info = load_audio(path)\n",
    "    cur_audio_input = cur_audio_input.unsqueeze(0)\n",
    "    \n",
    "    # projecting to 1024 input embedding dimension for T5\n",
    "    audio_proj = nn.Sequential(nn.LayerNorm(768, elementwise_affine=False), nn.Linear(768, 1024))\n",
    "    audio_input = audio_encoder(cur_audio_input)  # [B, 512, 768]\n",
    "    audio_input = audio_input.reshape(audio_input.shape[0], 8, 64, audio_input.shape[-1])\n",
    "    audio_input = torch.mean(audio_input, dim=1)  # mean pool over the frequency dimension # [B, 64, 768]\n",
    "    audio_input = torch.nn.functional.avg_pool2d(audio_input, (2, 1)) #[B, 32, 768]\n",
    "    # hard norm to 50\n",
    "    audio_input = audio_input / 50\n",
    "    audio_input = audio_proj(audio_input) #[B, 32, 1024]\n",
    "    \n",
    "    return audio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c664636-70a2-44d0-9451-b6a8c321ecf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abfcde6e-8a70-4175-8b7e-3fe3c7149121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42670ff5-a524-4846-9f6b-c93b944ed181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Label Labela       , alow...     ... ant...   ,  and melsonant...... ...,,  and ... ant... ,\n",
      "Label Label...:......;;;;\n",
      "Label Label Label'       ande:  and...          ...\n",
      "The thea    a shot      its    a   in ......  on  of gun theshots..\n",
      "The thea   s     special howa can    is the  and.\n",
      "The the  the... shaving... be   than may may than the sound sound constant sound of the razor shaving is be............. to.\n",
      "a  ... .\n",
      ".,, and,...... me...... was........................ might....................................\n",
      "Label Label Label...;;; );;... and and home music music music music music music music\n",
      "Label Label Label'       :   ly and...\n",
      "Label Label Labela         a  a c\n",
      "a        as., and .\n",
      "Label the'  tic      Soul..., ly... raw   ; ;... ;   ...\n",
      "The the the is  or dull. are...er dull lower frequencies are more as dull...\n",
      "audio  or...:.........;.....................;..................\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CombinedEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, model, device, max_length=512):\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instruction = self.data[idx]['instruction']\n",
    "        audio = return_audio(self.data[idx]['audio_id'])\n",
    "        label = self.data[idx]['output']\n",
    "        \n",
    "        input_ids = self.tokenizer(instruction, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        decoder_input_ids = self.tokenizer(label, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prompt_embeddings = self.model.shared(input_ids).squeeze(0)  # Shape: (sequence_length, 1024)\n",
    "\n",
    "        audio_embeddings = audio.to(self.device).squeeze(0)  # Ensure audio is already a tensor with shape (32, 1024)\n",
    "\n",
    "        combined_embeddings = torch.cat((prompt_embeddings, audio_embeddings), dim=0)  # Shape: (sequence_length + 32, 1024)\n",
    "\n",
    "        if combined_embeddings.size(0) > self.max_length:\n",
    "            combined_embeddings = combined_embeddings[:self.max_length, :]\n",
    "        \n",
    "\n",
    "        attention_mask = torch.ones(combined_embeddings.size(0)).to(self.device)\n",
    "\n",
    "\n",
    "        padding_length = self.max_length - combined_embeddings.size(0)\n",
    "        if padding_length > 0:\n",
    "            padding_tensor = torch.zeros((padding_length, combined_embeddings.size(1))).to(self.device)\n",
    "            combined_embeddings = torch.cat((combined_embeddings, padding_tensor), dim=0)\n",
    "            \n",
    "            padding_attention_mask = torch.zeros(padding_length).to(self.device)\n",
    "            attention_mask = torch.cat((attention_mask, padding_attention_mask))\n",
    "\n",
    "        return combined_embeddings, decoder_input_ids.squeeze(0), attention_mask\n",
    "\n",
    "def collate_fn(batch):\n",
    "    combined_embeddings = [item[0] for item in batch]\n",
    "    decoder_input_ids = [item[1] for item in batch]\n",
    "    attention_masks = [item[2] for item in batch]\n",
    "\n",
    "    combined_embeddings_padded = torch.stack(combined_embeddings)\n",
    "    decoder_input_ids_padded = pad_sequence(decoder_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks_padded = torch.stack(attention_masks)\n",
    "\n",
    "    return combined_embeddings_padded, decoder_input_ids_padded, attention_masks_padded\n",
    "\n",
    "\n",
    "dataset = CombinedEmbeddingsDataset(data, tokenizer, model, device)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d6e7478-a346-430f-b431-1ad6ece42a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.5058, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(10.2880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(8.4542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.6250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0872, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example labels (target sequence for training)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Iterate through the DataLoader\n",
    "    for i, (combined_embeddings, decoder_input_ids_padded, attention_masks_padded) in enumerate(itertools.islice(dataloader, 2)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs_embeds=combined_embeddings, attention_mask=attention_masks_padded,labels=decoder_input_ids_padded)\n",
    "        loss = outputs.loss\n",
    "        print(loss)\n",
    "        #predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70b4ebe8-5573-455b-8fe1-c888b007921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, \"first_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad61e18f-41af-4dcd-8fed-676f33666e6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/ltu-replication/lib/python3.12/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[1;32m   1026\u001b[0m                      map_location,\n\u001b[1;32m   1027\u001b[0m                      pickle_module,\n\u001b[1;32m   1028\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[1;32m   1029\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ltu-replication/lib/python3.12/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ltu-replication/lib/python3.12/site-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/ltu-replication/lib/python3.12/site-packages/torch/serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/ltu-replication/lib/python3.12/site-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/ltu-replication/lib/python3.12/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcuda(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/ltu-replication/lib/python3.12/site-packages/torch/_utils.py:114\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "checkpoint = torch.load(\"first_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429a4ed-867c-4a2c-8cb3-22beb5a44985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
