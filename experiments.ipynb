{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc84367a-149d-4e6e-ac91-dbc31c80ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import os, csv, argparse, wget\n",
    "from AST.models import ASTModel\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf75d7-5a77-4885-a48d-c82a98f6cd37",
   "metadata": {},
   "source": [
    " - 10-second audio waveform -> sequence of 128-dimensional long filterbank \n",
    " - 1024(time) x 128(frequency) spectogram\n",
    " - split into 512(64 time) x 8(frequency)) square patches of shape 16x16 fed into AST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65fcb3da-0d8e-45ee-94a5-3ad0a2377489",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filterbank\n",
    "def load_audio(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    audio_info = 'Original input audio length {:.2f} seconds, number of channels: {:d}, sampling rate: {:d}.'.format(waveform.shape[1]/sample_rate, waveform.shape[0], sample_rate)\n",
    "    if waveform.shape[0] != 1:\n",
    "        waveform = waveform[0].unsqueeze(0)\n",
    "        audio_info += ' Only the first channel is used.'\n",
    "    if sample_rate == 16000:\n",
    "        pass\n",
    "    else:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=sample_rate, new_freq=16000)\n",
    "        sample_rate = 16000\n",
    "        audio_info += ' Resample to 16000Hz.'\n",
    "    waveform = waveform - waveform.mean()\n",
    "    fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sample_rate,\n",
    "                                              use_energy=False, window_type='hanning',\n",
    "                                              num_mel_bins=128, dither=0.0, frame_shift=10)\n",
    "    target_length = 1024\n",
    "    n_frames = fbank.shape[0]\n",
    "    p = target_length - n_frames\n",
    "    if p > 0:\n",
    "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "        fbank = m(fbank)\n",
    "    elif p < 0:\n",
    "        fbank = fbank[0:target_length, :]\n",
    "    # normalize the fbank\n",
    "    fbank = (fbank + 5.081) / 4.4849\n",
    "    return fbank, audio_info\n",
    "    \n",
    "def load_label(label_csv):\n",
    "    with open(label_csv, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        lines = list(reader)\n",
    "    labels = []\n",
    "    ids = []  # Each label has a unique id such as \"/m/068hy\"\n",
    "    for i1 in range(1, len(lines)):\n",
    "        id = lines[i1][1]\n",
    "        label = lines[i1][2]\n",
    "        ids.append(id)\n",
    "        labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe9447d9-ea49-4b01-834f-10279178794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from timm.models.layers import to_2tuple, trunc_normal_, DropPath\n",
    "from timm.models.vision_transformer import Attention, Mlp, PatchEmbed, Block\n",
    "from pos_embed import get_2d_sincos_pos_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278a9cf-cf05-404a-93d6-6b390df4a1c7",
   "metadata": {},
   "source": [
    "### WORKING CODE START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "784b35d3-2167-4caf-be24-ff593c34f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.norm1_a = norm_layer(dim)\n",
    "        self.norm1_v = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.norm2_a = norm_layer(dim)\n",
    "        self.norm2_v = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, modality=None):\n",
    "        if modality == None:\n",
    "            x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        elif modality == 'a':\n",
    "            x = x + self.drop_path(self.attn(self.norm1_a(x)))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2_a(x)))\n",
    "        elif modality == 'v':\n",
    "            x = x + self.drop_path(self.attn(self.norm1_v(x)))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2_v(x)))\n",
    "\n",
    "        # this is a workaround to avoid ddp complain\n",
    "        x = x + 0.0 * (self.norm1(x) + self.norm2(x) + self.norm1_a(x) + self.norm2_a(x) + self.norm1_v(x) + self.norm2_v(x))\n",
    "        return x\n",
    "\n",
    "# the finetuned CAV-MAE model\n",
    "class CAVMAEFTAudio(nn.Module):\n",
    "    def __init__(self, img_size=224, audio_length=1024, patch_size=16, in_chans=3,\n",
    "                 embed_dim=768, modality_specific_depth=11, num_heads=12, mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False, tr_pos=True):\n",
    "        super().__init__()\n",
    "        timm.models.vision_transformer.Block = Block\n",
    "\n",
    "        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n",
    "        timm.models.vision_transformer.Block = Block\n",
    "\n",
    "        self.patch_embed_a = PatchEmbed(img_size, patch_size, 1, embed_dim)\n",
    "\n",
    "        self.patch_embed_a.num_patches = int(audio_length * 128 / 256)\n",
    "\n",
    "        self.modality_a = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        self.pos_embed_a = nn.Parameter(torch.zeros(1, self.patch_embed_a.num_patches, embed_dim), requires_grad=tr_pos)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks_a = nn.ModuleList([Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer) for i in range(modality_specific_depth)])\n",
    "        self.blocks_u = nn.ModuleList([Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer) for i in range(12 - modality_specific_depth)])\n",
    "\n",
    "        self.norm_a = norm_layer(embed_dim)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def get_patch_num(self, input_shape, stride):\n",
    "        test_input = torch.zeros(1, 1, input_shape[0], input_shape[1])\n",
    "        test_proj = torch.nn.Conv2d(1, 4, kernel_size=(16, 16), stride=(stride, stride))\n",
    "        test_output = test_proj(test_input)\n",
    "        return test_output.shape[2], test_output[3], test_output[2] * test_output[2]\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        pos_embed_a = get_2d_sincos_pos_embed(self.pos_embed_a.shape[-1], 8, int(self.patch_embed_a.num_patches/8), cls_token=False)\n",
    "        self.pos_embed_a.data.copy_(torch.from_numpy(pos_embed_a).float().unsqueeze(0))\n",
    "\n",
    "        w = self.patch_embed_a.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        torch.nn.init.normal_(self.modality_a, std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, a):\n",
    "        # expect input [b, t, f]\n",
    "        a = a.unsqueeze(1)\n",
    "        a = a.transpose(2, 3)\n",
    "        a = self.patch_embed_a(a)\n",
    "        a = a + self.pos_embed_a\n",
    "        a = a + self.modality_a\n",
    "\n",
    "        for blk in self.blocks_a:\n",
    "            a = blk(a)\n",
    "\n",
    "        for blk in self.blocks_u:\n",
    "            a = blk(a, 'a')\n",
    "\n",
    "        a = self.norm_a(a)\n",
    "        # output in shape [b, t, dim]\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69634c4-7a06-48c2-bc56-4b7aca17ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from prompter import Prompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88a8966-230a-49f3-8cbe-40fd8c4f6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"hello what are you? what can you do?\"\n",
    "response = \"I am a helpful AI assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "335641c8-c0e3-44b3-bddd-b8a037425805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MBZUAI/LaMini-T5-738M\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\"\"\"\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd644638-d2e6-4826-a11f-1131a4e603eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1754d1-26cd-4f7a-b196-ecf01e96e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416df75b-4d52-4eb1-82a4-7aa9bc15c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./data/sample/audio_samples/audio/5FTf2UXOjd8_000160.flac\"\n",
    "cur_audio_input, audio_info = load_audio(file)\n",
    "cur_audio_input = cur_audio_input.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09f16c70-33ec-4c58-a2e5-8eabbed46af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_encoder = CAVMAEFTAudio()\n",
    "# projecting to 1024 input embedding dimension for T5\n",
    "audio_proj = nn.Sequential(nn.LayerNorm(768, elementwise_affine=False), nn.Linear(768, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c998b341-942f-4696-a72d-f432f1d83e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_input = audio_encoder(cur_audio_input)  # [B, 512, 768]\n",
    "audio_input = audio_input.reshape(audio_input.shape[0], 8, 64, audio_input.shape[-1])\n",
    "audio_input = torch.mean(audio_input, dim=1)  # mean pool over the frequency dimension # [B, 64, 768]\n",
    "audio_input = torch.nn.functional.avg_pool2d(audio_input, (2, 1)) #[B, 32, 768]\n",
    "# hard norm to 50\n",
    "audio_input = audio_input / 50\n",
    "audio_input = audio_proj(audio_input) #[B, 32, 1024]\n",
    "audio_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7d7bfe-50e7-4415-a2d9-2707b461eeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 1024])\n",
      "torch.Size([1, 14, 1024])\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'can be inferered from this audio following right after thisn is is that the is  a guitar guitar,  guitar. is be inferred from the following sound ssonant sound of the guitar.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt_text = \"what can be infered from this audio following right after this\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_embeddings = model.shared(input_ids)  # Shape: (1, sequence_length, 1024)\n",
    "    prompt_embeddings = prompt_embeddings.to(device)\n",
    "\n",
    "\n",
    "target_text = \"what can be infered from this audio following right after this: The audio clip suggests that someone is playing a steel \"\\\n",
    "\"guitar or slide guitar. This can be inferred from the smooth and resonant sound of the instrument.\"\n",
    "\n",
    "target_ids = tokenizer(target_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "decoder_input_ids = model._shift_right(target_ids)\n",
    "\n",
    "audio_embeddings = audio_input.to(device)  # Shape: (1, 32, 1024)\n",
    "\n",
    "print(audio_embeddings.shape)\n",
    "print(prompt_embeddings.shape)\n",
    "# Concatenate prompt and audio embeddings\n",
    "combined_embeddings = torch.cat((prompt_embeddings, audio_embeddings), dim=1)  # Shape: (1, sequence_length + 32, 1024)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "\n",
    "if combined_embeddings.size(1) > max_length:\n",
    "    combined_embeddings = combined_embeddings[:, :max_length, :]\n",
    "\n",
    "padding_length = max_length - combined_embeddings.size(1)\n",
    "if padding_length > 0:\n",
    "    padding_tensor = torch.zeros((combined_embeddings.size(0), padding_length, combined_embeddings.size(2))).to(device)\n",
    "    combined_embeddings = torch.cat((combined_embeddings, padding_tensor), dim=1)\n",
    "\n",
    "\n",
    "attention_mask = torch.ones(combined_embeddings.size(0), combined_embeddings.size(1)).to(device)\n",
    "if padding_length > 0:\n",
    "    attention_mask[:, -padding_length:] = 0\n",
    "\n",
    "\n",
    "outputs = model(inputs_embeds=combined_embeddings, attention_mask=attention_mask,decoder_input_ids=decoder_input_ids)\n",
    "print(outputs.loss)\n",
    "logits = outputs.logits  \n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "decoded_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02de14-b75b-460e-a3e7-5feb2e903bf5",
   "metadata": {},
   "source": [
    "# WORKING CODE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28fe9c-7f3a-4b0f-8459-c08531161bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e4ea786-8fdb-4461-91e8-81a70999da0d",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846434a5-cfe5-4b38-a5f7-efc05e097435",
   "metadata": {},
   "source": [
    "\n",
    "#### Example workings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "223b44d8-bcba-4a9b-9aed-0b77d9ea4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "# preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n",
    "# This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n",
    "decoder_input_ids = model._shift_right(decoder_input_ids)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dd3e18f-69c1-41b4-b6cb-8a4b08df093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predice results:\n",
      "- Music: 0.9717\n",
      "- Guitar: 0.6226\n",
      "- Musical instrument: 0.5547\n",
      "- Plucked string instrument: 0.5020\n",
      "- Steel guitar, slide guitar: 0.0828\n",
      "- Bass guitar: 0.0827\n",
      "- Tapping (guitar technique): 0.0817\n",
      "- Strum: 0.0570\n",
      "- Acoustic guitar: 0.0519\n",
      "- Electric guitar: 0.0502\n",
      "Listen to this sample: \n",
      "(527,)\n"
     ]
    }
   ],
   "source": [
    "# Load AST model with AudioSet pretrained weights\n",
    "\n",
    "checkpoint_path = \"./data/audioset_0.4593.pth\"\n",
    "ast = ASTModel(label_dim=527, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False)\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n",
    "audio_model = torch.nn.DataParallel(ast, device_ids=[0])\n",
    "audio_model.load_state_dict(checkpoint)\n",
    "audio_model = audio_model.to(torch.device(\"cuda:0\"))\n",
    "audio_model.eval()\n",
    "\n",
    "label_csv = \"./data/class_labels_indices.csv\"\n",
    "labels = load_label(label_csv)\n",
    "\n",
    "with torch.no_grad():\n",
    "  with autocast():\n",
    "    output = audio_model.forward(fb_data)\n",
    "    output = torch.sigmoid(output)\n",
    "result_output = output.data.cpu().numpy()[0]\n",
    "sorted_indexes = np.argsort(result_output)[::-1]\n",
    "# Print audio tagging top probabilities\n",
    "print('Predice results:')\n",
    "for k in range(10):\n",
    "    print('- {}: {:.4f}'.format(np.array(labels)[sorted_indexes[k]], result_output[sorted_indexes[k]]))\n",
    "print('Listen to this sample: ')\n",
    "IPython.display.Audio('./data/sample/audio_samples/audio/5FTf2UXOjd8_000160.flac', rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "251db776-58a6-4d64-839c-8b1787495ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f531f7a-4c3d-4848-864d-7e03d6da8fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8162fd6-8d69-4805-8724-8141487f4c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a833810-e74e-4178-9e87-ae84b3b95f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be45797-209b-499f-8f90-0bd10e030982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d2f76-ef0a-4935-b58c-e5a267d37322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "40cfc772-d6dc-4360-965a-66ebdc768577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from prompter import Prompter\n",
    "data_path = \"./openaqa_toy.json\"\n",
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "tokenizer.pad_token_id = (\n",
    "        0  # unk. we want this to be different from the eos token\n",
    "    )\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"input\"],\n",
    "        data_point[\"output\"]\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"], data_point[\"input\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47c56318-42c6-4006-ad86-7915f4b82e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.598347663879395"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89d892ec-cfa4-4127-bd2e-9855705e0e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 768\n",
    "max_target_length = 768\n",
    "input_sequence_1 = \"Welcome to NYC\"\n",
    "output_sequence_1 = \"Bienvenue à NYC\"\n",
    "\n",
    "input_sequence_2 = \"HuggingFace is a company\"\n",
    "output_sequence_2 = \"HuggingFace est une entreprise\"\n",
    "\n",
    "# encode the inputs\n",
    "task_prefix = \"translate English to French: \"\n",
    "input_sequences = [input_sequence_1, input_sequence_2]\n",
    "\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "115762a8-df56-4251-a1f2-6a7aa10e4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ab02ab6-3064-40fa-9cbe-fe2c4fcc5d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2628182172775269"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_encoding = tokenizer(\n",
    "    [output_sequence_1, output_sequence_2],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "labels = target_encoding.input_ids\n",
    "\n",
    "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "# forward pass\n",
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d9739-9ccf-4d0f-a81e-0c4f65444b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
